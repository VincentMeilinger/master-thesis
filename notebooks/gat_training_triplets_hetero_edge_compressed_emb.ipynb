{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-04T17:17:34.143042Z",
     "start_time": "2024-11-04T17:17:34.140304Z"
    }
   },
   "source": [
    "from notebooks.util import GraphTripletDataset\n",
    "from util import *\n",
    "from gat_models import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn.modules.loss import TripletMarginLoss\n",
    "\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.shared.graph_schema import *\n",
    "from src.shared.graph_sampling import GraphSampling\n",
    "\n",
    "random.seed(40)\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(40)\n",
    "torch.cuda.manual_seed_all(40)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configurations",
   "id": "ee4fa502d7e51e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:17:34.163708Z",
     "start_time": "2024-11-04T17:17:34.154529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph sampling configurations\n",
    "node_spec = [\n",
    "    NodeType.PUBLICATION,\n",
    "]\n",
    "\n",
    "edge_spec = [\n",
    "    EdgeType.SIM_VENUE,\n",
    "    EdgeType.SIM_ABSTRACT,\n",
    "    EdgeType.SIM_AUTHOR,\n",
    "]\n",
    "\n",
    "node_properties = [\n",
    "    'id',\n",
    "    'title',\n",
    "    'abstract',\n",
    "    'title_emb',\n",
    "    'abstract_emb',\n",
    "    'feature_vec',\n",
    "]\n",
    "\n",
    "database = 'homogeneous-graph-compressed-emb'\n",
    "gs = GraphSampling(\n",
    "    node_spec=node_spec,\n",
    "    edge_spec=edge_spec,\n",
    "    node_properties=node_properties,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "# Model configurations\n",
    "\n",
    "config = {\n",
    "    'experiment': 'GATv2 encoder (with linear layer + dropout) trained on heterogeneous graph (publication nodes with title and abstract, similarity and co-author edges) using Triplet Loss and dimension reduced embeddings',\n",
    "    'max_hops': 3,\n",
    "    'model_node_feature': 'feature_vec',  # Node feature to use for GAT encoder\n",
    "    'hidden_channels': 64,\n",
    "    'out_channels': 16,\n",
    "    'num_heads': 8,\n",
    "    'margin': 1.0,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 0.005,\n",
    "    'weight_decay': 5e-4,\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "model_class = HeteroGATEncoderLinear\n",
    "loss_fn = TripletMarginLoss(margin=config['margin'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_file_postfix = \"triplets_hetero_edges_compressed_emb_linear_layer\""
   ],
   "id": "91b04efb689d61f4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training functions",
   "id": "c0e4399d4af30692"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:17:34.206436Z",
     "start_time": "2024-11-04T17:17:34.199946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, batch_anchor, batch_pos, batch_neg, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_anchor = batch_anchor.to(device)\n",
    "    batch_pos = batch_pos.to(device)\n",
    "    batch_neg = batch_neg.to(device)\n",
    "\n",
    "    emb_a = model(batch_anchor)\n",
    "    emb_p = model(batch_pos)\n",
    "    emb_n = model(batch_neg)\n",
    "    \n",
    "    emb_a_central = emb_a[NodeType.PUBLICATION.value][batch_anchor.central_node_id]\n",
    "    emb_p_central = emb_p[NodeType.PUBLICATION.value][batch_pos.central_node_id]\n",
    "    emb_n_central = emb_n[NodeType.PUBLICATION.value][batch_neg.central_node_id]\n",
    "    \n",
    "    loss = loss_fn(emb_a_central, emb_p_central, emb_n_central)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    batch_loss = loss.item()\n",
    "    #print(f\"Batch loss: {batch_loss:.4f}\")\n",
    "    return batch_loss\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_anchor, batch_pos, batch_neg in dataloader:\n",
    "            batch_anchor = batch_anchor.to(device)\n",
    "            batch_pos = batch_pos.to(device)\n",
    "            batch_neg = batch_neg.to(device)\n",
    "    \n",
    "            emb_a = model(batch_anchor)\n",
    "            emb_p = model(batch_pos)\n",
    "            emb_n = model(batch_neg)\n",
    "            \n",
    "            emb_a_central = emb_a[NodeType.PUBLICATION.value][batch_anchor.central_node_id]\n",
    "            emb_p_central = emb_p[NodeType.PUBLICATION.value][batch_pos.central_node_id]\n",
    "            emb_n_central = emb_n[NodeType.PUBLICATION.value][batch_neg.central_node_id]\n",
    "            \n",
    "            loss = loss_fn(emb_a_central, emb_p_central, emb_n_central)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Compute average loss    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_num_correct = 0\n",
    "    total_pos_correct = 0\n",
    "    total_neg_correct = 0\n",
    "    total_num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_anchor, batch_pos, batch_neg in dataloader:\n",
    "            batch_anchor = batch_anchor.to(device)\n",
    "            batch_pos = batch_pos.to(device)\n",
    "            batch_neg = batch_neg.to(device)\n",
    "\n",
    "            emb_a = model(batch_anchor)\n",
    "            emb_p = model(batch_pos)\n",
    "            emb_n = model(batch_neg)\n",
    "\n",
    "            emb_a_central = emb_a[NodeType.PUBLICATION.value][batch_anchor.central_node_id]\n",
    "            emb_p_central = emb_p[NodeType.PUBLICATION.value][batch_pos.central_node_id]\n",
    "            emb_n_central = emb_n[NodeType.PUBLICATION.value][batch_neg.central_node_id]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(emb_a_central, emb_p_central, emb_n_central)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute distances\n",
    "            d_ap = F.pairwise_distance(emb_a_central, emb_p_central)\n",
    "            d_an = F.pairwise_distance(emb_a_central, emb_n_central)\n",
    "\n",
    "            # Determine correct predictions based on margin\n",
    "            correct_pos = (d_ap < config['margin']).cpu()\n",
    "            correct_neg = (d_an > config['margin']).cpu()\n",
    "\n",
    "            # Sum up correct predictions\n",
    "            num_correct_pos = correct_pos.sum().item()\n",
    "            num_correct_neg = correct_neg.sum().item()\n",
    "            num_correct = num_correct_pos + num_correct_neg\n",
    "\n",
    "            total_num_correct += num_correct\n",
    "            total_pos_correct += num_correct_pos\n",
    "            total_neg_correct += num_correct_neg\n",
    "            total_num_samples += len(batch_anchor)\n",
    "\n",
    "    # Compute averages\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_correct_pos = total_pos_correct / total_num_samples\n",
    "    avg_correct_neg = total_neg_correct / total_num_samples\n",
    "    avg_num_correct = total_num_correct / (2 * total_num_samples)  # Since we have two conditions\n",
    "\n",
    "    print(f\"Correct positive: {total_pos_correct} ({avg_correct_pos * 100:.2f}%), Correct negative: {total_neg_correct} ({avg_correct_neg * 100:.2f}%)\")\n",
    "    print(f\"Total correct: {total_num_correct} ({avg_num_correct * 100:.2f}%)\")\n",
    "    print(f\"Eval Loss: {avg_loss:.4f}, Eval Accuracy: {avg_num_correct:.4f}\")\n",
    "\n",
    "    return avg_loss, avg_num_correct, avg_correct_pos, avg_correct_neg\n",
    "            "
   ],
   "id": "2ff2cd9e6fd091ef",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Configuration",
   "id": "b67cf89d3d17cc12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:20:53.967129Z",
     "start_time": "2024-11-04T17:17:34.248664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DatabaseWrapper(database=database)\n",
    "data_harvester = TripletDataHarvester(db=db, gs=gs, edge_spec=edge_spec, config=config, save_file_postfix=save_file_postfix)\n",
    "\n",
    "\n",
    "# Split the pairs into train and test\n",
    "\n",
    "train_size = int(0.85 * len(data_harvester.triplets))\n",
    "test_size = int(0.1 * len(data_harvester.triplets))\n",
    "eval_size = len(data_harvester.triplets) - train_size - test_size\n",
    "\n",
    "# Harvest the evaluation triplets first, since triplets are ordered by author. This will ensure that the evaluation set has authors not seen in the training set.\n",
    "eval_triplets = data_harvester.triplets[:eval_size]\n",
    "\n",
    "train_test_triplets = data_harvester.triplets[eval_size:]\n",
    "random.shuffle(train_test_triplets)\n",
    "\n",
    "train_triplets = train_test_triplets[:train_size]\n",
    "test_triplets = train_test_triplets[train_size:]\n",
    "config['train_size'] = len(train_triplets)\n",
    "config['test_size'] = len(test_triplets)\n",
    "config['eval_size'] = len(eval_triplets)\n",
    "\n",
    "print(f\"Train size: {len(train_triplets)}, Test size: {len(test_triplets)}, Eval size: {len(eval_triplets)}\")\n",
    "\n",
    "# Create the datasets from the pairs (distinct pairs for training and testing)\n",
    "train_dataset = GraphTripletDataset(train_triplets, gs, config=config)\n",
    "test_dataset = GraphTripletDataset(test_triplets, gs, config=config)\n",
    "eval_dataset = GraphTripletDataset(eval_triplets, gs, config=config)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=custom_triplet_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=custom_triplet_collate)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=custom_triplet_collate)\n",
    "\n",
    "# Create model\n",
    "metadata = (\n",
    "    [n.value for n in node_spec],\n",
    "    [edge_pyg_key_vals[r] for r in edge_spec]\n",
    ")\n",
    "config['node_spec'] = metadata[0]\n",
    "config['edge_spec'] = metadata[1]\n",
    "model = model_class(metadata, config['hidden_channels'], config['out_channels'], num_heads=config['num_heads']).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])"
   ],
   "id": "62ef1028a5c5fd5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 18:17:34,251 - DatabaseWrapper - INFO - Connecting to the database ...\n",
      "2024-11-04 18:17:34,252 - DatabaseWrapper - INFO - Database ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing triplets...\n",
      "Loading triplets...\n",
      "Could not load triplets from file. Generating triplets...\n",
      "Checking data validity...\n",
      "Out of 20034 checked papers, 12937 are valid and 7097 are invalid.\n",
      "Preparing pairs...\n",
      "Total triplets: 11755. Done.\n",
      "Generated 11755 triplets.\n",
      "Saving triplets...\n",
      "Triplets saved.\n",
      "Train size: 9991, Test size: 1175, Eval size: 589\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Loop",
   "id": "bdf9a36f9d9a6f43"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-04T17:20:54.019214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = config['num_epochs']\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "eval_losses = []\n",
    "eval_accuracies = []\n",
    "eval_correct_pos = []\n",
    "eval_correct_neg = []\n",
    "\n",
    "current_batch = 1\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"=== Epoch {epoch}/{num_epochs} ======================\")\n",
    "    epoch_marker_pos = list(range(0, len(train_dataloader) * epoch, len(train_dataloader)))\n",
    "    current_batch = 1\n",
    "    for batch_anchor, batch_pos, batch_neg in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        if batch_anchor is None or batch_pos is None or batch_neg is None:\n",
    "            continue\n",
    "        \n",
    "        if len(train_losses) % 10 == 0:\n",
    "            print(f\"___ Batch {current_batch}/{len(train_dataloader)} _________________________\")\n",
    "            # Model testing\n",
    "            test_loss = test(model, test_dataloader)\n",
    "            test_losses.append(test_loss)\n",
    "            test_epoch_marker_pos = [marker/10 for marker in epoch_marker_pos if marker != 0]\n",
    "            plot_loss(test_losses, epoch_marker_pos=test_epoch_marker_pos, plot_title='Test Loss', plot_avg=True, plot_file=f'./data/losses/test_loss_{save_file_postfix}.png')\n",
    "            \n",
    "            # Model evaluation\n",
    "            eval_loss, eval_num_correct, eval_correct_pos_val, eval_correct_neg_val = evaluate(model, eval_dataloader)\n",
    "            eval_losses.append(eval_loss)\n",
    "            eval_accuracies.append(eval_num_correct)\n",
    "            eval_correct_pos.append(eval_correct_pos_val)\n",
    "            eval_correct_neg.append(eval_correct_neg_val)\n",
    "            \n",
    "            plot_loss(eval_losses, epoch_marker_pos=test_epoch_marker_pos, plot_title='Evaluation Loss', plot_avg=True, plot_file=f'./data/losses/eval_loss_{save_file_postfix}.png')\n",
    "            plot_loss(eval_accuracies, epoch_marker_pos=test_epoch_marker_pos, plot_title='Evaluation Accuracy', plot_avg=False, plot_file=f'./data/losses/eval_accuracy_{save_file_postfix}.png')\n",
    "            \n",
    "        loss = train(model, batch_anchor, batch_pos, batch_neg, optimizer)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        plot_loss(train_losses, epoch_marker_pos=epoch_marker_pos, plot_title='Training Loss', plot_avg=True, plot_file=f'./data/losses/train_loss_{save_file_postfix}.png')\n",
    "        current_batch += 1\n",
    "        \n",
    "    # Save config and training results\n",
    "    eval_results = {\n",
    "        'eval_losses': eval_losses,\n",
    "        'eval_accuracies': eval_accuracies,\n",
    "        'eval_correct_pos': eval_correct_pos,\n",
    "        'eval_correct_neg': eval_correct_neg\n",
    "    }\n",
    "    save_training_results(train_losses, test_losses, eval_results, config, f'./data/results/training_results_{save_file_postfix}.json')\n",
    "    \n",
    "    # Save model if loss has decreased\n",
    "    if len(test_losses) > 1 and test_losses[-1] < min(test_losses[:-1]):\n",
    "        print(f\"Saving model at epoch {epoch}...\")\n",
    "        torch.save(model.state_dict(), f'./data/models/gat_encoder_{save_file_postfix}.pt')"
   ],
   "id": "c3b28cd52881796",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/20 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/313 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be21d6998155436c9708cc546ed79dab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Batch 1/313 _________________________\n",
      "Test Loss: 1.0114\n",
      "Correct positive: 589 (100.00%), Correct negative: 0 (0.00%)\n",
      "Total correct: 589 (50.00%)\n",
      "Eval Loss: 1.0111, Eval Accuracy: 0.5000\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
