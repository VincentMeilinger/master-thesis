{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T09:37:34.942672Z",
     "start_time": "2024-12-05T09:37:32.079074Z"
    }
   },
   "source": [
    "from util_homogeneous import *\n",
    "from util import *\n",
    "from training_homogeneous import *\n",
    "from gat_models import *\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.modules.loss import TripletMarginLoss\n",
    "\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.shared.graph_schema import *\n",
    "from src.shared.graph_sampling import GraphSampling\n",
    "\n",
    "random.seed(40)\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(40)\n",
    "torch.cuda.manual_seed_all(40)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincie/.anaconda3/envs/master/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "ee4fa502d7e51e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T09:37:34.956400Z",
     "start_time": "2024-12-05T09:37:34.952630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph sampling configurations\n",
    "node_spec = NodeType.PUBLICATION\n",
    "\n",
    "edge_spec = EdgeType.SIM_ORG\n",
    "\n",
    "node_properties = [\n",
    "    'id',\n",
    "    'feature_vec',\n",
    "]\n",
    "\n",
    "database = 'small-graph'\n",
    "gs = GraphSampling(\n",
    "    node_spec=[node_spec],\n",
    "    edge_spec=[edge_spec],\n",
    "    node_properties=node_properties,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "# Model configurations\n",
    "\n",
    "config = {\n",
    "    'experiment': 'GATv2 encoder (with linear layer + dropout) trained on small graph (publication nodes with title and abstract, org edges) using Triplet Loss and full embeddings',\n",
    "    'max_hops': 2,\n",
    "    'model_node_feature': 'feature_vec',  # Node feature to use for GAT encoder\n",
    "    'hidden_channels': 64,\n",
    "    'out_channels': 16,\n",
    "    'num_heads': 8,\n",
    "    'dropout_p': 0.4,\n",
    "    'margin': 1.0,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 0.005,\n",
    "    'weight_decay': 5e-4,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "model_class = HomoGATv2Encoder\n",
    "loss_fn = TripletMarginLoss(margin=config['margin'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# TODO: Adjust result folder name!\n",
    "result_folder_name = 'homogeneous (org) full_emb linear_layer dropout'\n",
    "result_folder_path = f'./data/results/{result_folder_name}'\n",
    "if not os.path.exists(result_folder_path):\n",
    "    os.mkdir(result_folder_path)"
   ],
   "id": "91b04efb689d61f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default edge type: SimilarOrg for homogeneous graph sampling.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T09:40:37.397790Z",
     "start_time": "2024-12-05T09:37:35.001006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DatabaseWrapper(database=database)\n",
    "data_harvester = TripletDataHarvester(db=db, gs=gs, edge_spec=[edge_spec], config=config, valid_triplets_save_file='valid_triplets_homogeneous_org_small_graph', transformer_model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Split the pairs into train and test\n",
    "\n",
    "train_size = int(0.85 * len(data_harvester.triplets))\n",
    "test_size = int(0.1 * len(data_harvester.triplets))\n",
    "eval_size = len(data_harvester.triplets) - train_size - test_size\n",
    "\n",
    "# Harvest the evaluation triplets first, since triplets are ordered by author. This will ensure that the evaluation set has authors not seen in the training set.\n",
    "eval_triplets = data_harvester.triplets[:eval_size]\n",
    "\n",
    "train_test_triplets = data_harvester.triplets[eval_size:]\n",
    "random.shuffle(train_test_triplets)\n",
    "\n",
    "train_triplets = train_test_triplets[:train_size]\n",
    "test_triplets = train_test_triplets[train_size:]\n",
    "config['train_size'] = len(train_triplets)\n",
    "config['test_size'] = len(test_triplets)\n",
    "config['eval_size'] = len(eval_triplets)\n",
    "\n",
    "print(f\"Train size: {len(train_triplets)}, Test size: {len(test_triplets)}, Eval size: {len(eval_triplets)}\")\n",
    "\n",
    "# Create the datasets from the pairs (distinct pairs for training and testing)\n",
    "train_dataset = HomogeneousGraphTripletDataset(train_triplets, gs, config=config)\n",
    "test_dataset = HomogeneousGraphTripletDataset(test_triplets, gs, config=config)\n",
    "eval_dataset = HomogeneousGraphTripletDataset(eval_triplets, gs, config=config)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=custom_triplet_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=custom_triplet_collate)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=custom_triplet_collate)\n",
    "\n",
    "# Create model\n",
    "metadata = (\n",
    "    node_spec.value,\n",
    "    edge_spec.value\n",
    ")\n",
    "config['node_spec'] = metadata[0]\n",
    "config['edge_spec'] = metadata[1]\n",
    "model = model_class(config['hidden_channels'], config['out_channels'], num_heads=config['num_heads'], dropout_p=config['dropout_p']).to(device)\n",
    "#optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])"
   ],
   "id": "62ef1028a5c5fd5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:37:35,003 - DatabaseWrapper - INFO - Connecting to the database ...\n",
      "2024-12-05 10:37:35,004 - DatabaseWrapper - INFO - Database ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing triplets...\n",
      "Loading triplets...\n",
      "Could not load triplets from file. Generating triplets...\n",
      "Checking data validity...\n",
      "Out of 8865 checked papers, 2854 are valid and 6011 are invalid.\n",
      "Generating hard triplets ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincie/.anaconda3/envs/master/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too few triplets generated: 1842. Generating more triplets...\n",
      "Total triplets generated: 3684. Done.\n",
      "Generated 3684 triplets.\n",
      "Saving triplets...\n",
      "Triplets saved.\n",
      "Train size: 3131, Test size: 368, Eval size: 185\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-05T09:40:37.407034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = config['num_epochs']\n",
    "results = defaultdict(list)\n",
    "\n",
    "current_batch = 1\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"=== Epoch {epoch}/{num_epochs} ======================\")\n",
    "    epoch_marker_pos = list(range(0, len(train_dataloader) * epoch, len(train_dataloader)))\n",
    "    current_batch = 1\n",
    "    for batch_anchor, batch_pos, batch_neg in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        if batch_anchor is None or batch_pos is None or batch_neg is None:\n",
    "            continue\n",
    "        \n",
    "        if current_batch == 1 or current_batch == len(train_dataloader) // 2:\n",
    "            print(f\"___ Current Batch {current_batch}/{len(train_dataloader)} _________________________\")\n",
    "            # Model testing\n",
    "            print(\"    Test Results:\")\n",
    "            test_loss, test_num_correct, test_correct_pos_val, test_correct_neg_val, test_precision, test_recall, test_F1 = test_and_eval(\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=test_dataloader,\n",
    "                margin=config['margin']\n",
    "            )\n",
    "            results['test_total_loss'].append(test_loss)\n",
    "            results['test_accuracies'].append(test_num_correct)\n",
    "            results['test_accuracies_correct_pos'].append(test_correct_pos_val)\n",
    "            results['test_accuracies_correct_neg'].append(test_correct_neg_val)\n",
    "            results['test_precision'].append(test_precision)\n",
    "            results['test_recall'].append(test_recall)\n",
    "            results['test_F1'].append(test_F1)\n",
    "    \n",
    "            plot_losses(\n",
    "                losses=[results['test_total_loss']], \n",
    "                epoch_len=2, \n",
    "                plot_title='Test Loss', \n",
    "                plot_file=result_folder_path + '/test_loss.png', \n",
    "                line_labels=[\"Triplet Loss\"]\n",
    "            )\n",
    "            \n",
    "            plot_losses(\n",
    "                [results['test_accuracies'], results['test_accuracies_correct_pos'], results['test_accuracies_correct_neg']],\n",
    "                epoch_len=2,\n",
    "                plot_title='Test Accuracy',\n",
    "                x_label='Test Iterations',\n",
    "                y_label='Accuracy',\n",
    "                line_labels=['Total Accuracy', 'Correct Pos', 'Correct Neg'],\n",
    "                plot_file=result_folder_path + '/test_accuracy.png'\n",
    "            )\n",
    "            \n",
    "            # Model evaluation\n",
    "            print(\"    Eval Results:\")\n",
    "            eval_loss, eval_num_correct, eval_correct_pos_val, eval_correct_neg_val, eval_precision, eval_recall, eval_F1 = test_and_eval(\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=eval_dataloader,\n",
    "                margin=config['margin']\n",
    "            )\n",
    "            results['eval_total_loss'].append(eval_loss)\n",
    "            results['eval_accuracies'].append(eval_num_correct)\n",
    "            results['eval_accuracies_correct_pos'].append(eval_correct_pos_val)\n",
    "            results['eval_accuracies_correct_neg'].append(eval_correct_neg_val)\n",
    "            results['eval_precision'].append(eval_precision)\n",
    "            results['eval_recall'].append(eval_recall)\n",
    "            results['eval_F1'].append(eval_F1)\n",
    "            \n",
    "            plot_losses(\n",
    "                losses=[results['eval_total_loss']], \n",
    "                epoch_len=2, \n",
    "                plot_title='Evaluation Loss', \n",
    "                plot_file=result_folder_path + '/eval_loss.png', \n",
    "                line_labels=[\"Triplet Loss\"]\n",
    "            )\n",
    "            \n",
    "            plot_losses(\n",
    "                [results['eval_accuracies'], results['eval_accuracies_correct_pos'], results['eval_accuracies_correct_neg']], \n",
    "                epoch_len=2, \n",
    "                plot_title='Evaluation Accuracy', \n",
    "                x_label='Eval Iterations',\n",
    "                y_label='Accuracy',\n",
    "                line_labels=['Total Accuracy', 'Correct Pos', 'Correct Neg'],\n",
    "                plot_file=result_folder_path + '/eval_accuracy.png'\n",
    "            )\n",
    "            \n",
    "            # Save config and training results\n",
    "            config['results'] = results\n",
    "            save_dict_to_json(config, result_folder_path + '/training_data.json')\n",
    "            \n",
    "        loss = train(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            batch_anchor=batch_anchor,\n",
    "            batch_pos=batch_pos,\n",
    "            batch_neg=batch_neg,\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        results['train_loss'].append(loss)\n",
    "        \n",
    "        plot_loss(results['train_loss'], epoch_len=len(train_dataloader), plot_title='Training Loss', plot_avg=True, plot_file=result_folder_path + '/train_loss.png')\n",
    "        current_batch += 1\n",
    "        \n",
    "    # Save model if loss has decreased\n",
    "    if len(results['test_total_loss']) > 1 and results['test_total_loss'][-1] < min(results['test_total_loss'][:-1]):\n",
    "        print(f\"Saving model at epoch {epoch}...\")\n",
    "        torch.save(model.state_dict(), result_folder_path + '/gat_encoder.pt')"
   ],
   "id": "c3b28cd52881796",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/10 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62d728145033431291d737994e036164"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Current Batch 1/98 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 368 (100.00%), Correct negative: 0 (0.00%)\n",
      "        Total correct: 368 (50.00%)\n",
      "        Test/Eval Loss: 0.8982, Test/Eval Accuracy: 0.5000\n",
      "        Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "    Eval Results:\n",
      "        Correct positive: 185 (100.00%), Correct negative: 0 (0.00%)\n",
      "        Total correct: 185 (50.00%)\n",
      "        Test/Eval Loss: 0.8931, Test/Eval Accuracy: 0.5000\n",
      "        Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "___ Current Batch 49/98 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 323 (87.77%), Correct negative: 308 (83.70%)\n",
      "        Total correct: 631 (85.73%)\n",
      "        Test/Eval Loss: 0.2615, Test/Eval Accuracy: 0.8573\n",
      "        Precision: 0.8433, Recall: 0.8777, F1: 0.8602\n",
      "    Eval Results:\n",
      "        Correct positive: 136 (73.51%), Correct negative: 166 (89.73%)\n",
      "        Total correct: 302 (81.62%)\n",
      "        Test/Eval Loss: 0.3672, Test/Eval Accuracy: 0.8162\n",
      "        Precision: 0.8774, Recall: 0.7351, F1: 0.8000\n",
      "Saving model at epoch 1...\n",
      "=== Epoch 2/10 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b30ca6cc529406dad9b3495a8c748ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Current Batch 1/98 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 316 (85.87%), Correct negative: 323 (87.77%)\n",
      "        Total correct: 639 (86.82%)\n",
      "        Test/Eval Loss: 0.2538, Test/Eval Accuracy: 0.8682\n",
      "        Precision: 0.8753, Recall: 0.8587, F1: 0.8669\n",
      "    Eval Results:\n",
      "        Correct positive: 131 (70.81%), Correct negative: 141 (76.22%)\n",
      "        Total correct: 272 (73.51%)\n",
      "        Test/Eval Loss: 0.4207, Test/Eval Accuracy: 0.7351\n",
      "        Precision: 0.7486, Recall: 0.7081, F1: 0.7278\n",
      "___ Current Batch 49/98 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 331 (89.95%), Correct negative: 302 (82.07%)\n",
      "        Total correct: 633 (86.01%)\n",
      "        Test/Eval Loss: 0.2489, Test/Eval Accuracy: 0.8601\n",
      "        Precision: 0.8338, Recall: 0.8995, F1: 0.8654\n",
      "    Eval Results:\n",
      "        Correct positive: 149 (80.54%), Correct negative: 135 (72.97%)\n",
      "        Total correct: 284 (76.76%)\n",
      "        Test/Eval Loss: 0.3929, Test/Eval Accuracy: 0.7676\n",
      "        Precision: 0.7487, Recall: 0.8054, F1: 0.7760\n",
      "Saving model at epoch 2...\n",
      "=== Epoch 3/10 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2ef7ea0076a4bb286fe871d3d64da9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Current Batch 1/98 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 322 (87.50%), Correct negative: 313 (85.05%)\n",
      "        Total correct: 635 (86.28%)\n",
      "        Test/Eval Loss: 0.2415, Test/Eval Accuracy: 0.8628\n",
      "        Precision: 0.8541, Recall: 0.8750, F1: 0.8644\n",
      "    Eval Results:\n",
      "        Correct positive: 135 (72.97%), Correct negative: 161 (87.03%)\n",
      "        Total correct: 296 (80.00%)\n",
      "        Test/Eval Loss: 0.3793, Test/Eval Accuracy: 0.8000\n",
      "        Precision: 0.8491, Recall: 0.7297, F1: 0.7849\n",
      "___ Current Batch 49/98 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 319 (86.68%), Correct negative: 318 (86.41%)\n",
      "        Total correct: 637 (86.55%)\n",
      "        Test/Eval Loss: 0.2447, Test/Eval Accuracy: 0.8655\n",
      "        Precision: 0.8645, Recall: 0.8668, F1: 0.8657\n",
      "    Eval Results:\n",
      "        Correct positive: 146 (78.92%), Correct negative: 166 (89.73%)\n",
      "        Total correct: 312 (84.32%)\n",
      "        Test/Eval Loss: 0.2945, Test/Eval Accuracy: 0.8432\n",
      "        Precision: 0.8848, Recall: 0.7892, F1: 0.8343\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
