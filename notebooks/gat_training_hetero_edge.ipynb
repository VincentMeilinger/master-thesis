{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.637680Z",
     "start_time": "2024-10-13T10:18:54.633430Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "from typing import Any\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.distributions.constraints import positive\n",
    "from torch_geometric.nn.models.dimenet import triplets\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j import GraphDatabase\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, global_mean_pool, HeteroConv\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.data import HeteroData\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.nn.modules.loss import TripletMarginLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Batch\n",
    "from torch.optim import Adam\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.datasets.who_is_who import WhoIsWhoDataset\n",
    "from src.model.GAT.gat_encoder import GATv2Encoder\n",
    "from src.model.GAT.gat_decoder import GATv2Decoder\n",
    "from src.shared.graph_schema import *\n",
    "from src.model.loss.triplet_loss import TripletLoss\n",
    "from src.shared import config\n",
    "from torch.utils.data import Dataset\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from itertools import combinations, product\n",
    "from src.shared.graph_sampling import GraphSampling\n",
    "\n",
    "random.seed(40)\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(40)\n",
    "torch.cuda.manual_seed_all(40)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "ee4fa502d7e51e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.693677Z",
     "start_time": "2024-10-13T10:18:54.690337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph sampling configurations\n",
    "node_spec = [\n",
    "    NodeType.PUBLICATION,\n",
    "]\n",
    "\n",
    "edge_spec = [\n",
    "    EdgeType.SIM_TITLE,\n",
    "    EdgeType.SIM_ABSTRACT,\n",
    "    EdgeType.SIM_VENUE,\n",
    "    EdgeType.SIM_AUTHOR,\n",
    "]\n",
    "\n",
    "node_properties = [\n",
    "    'id',\n",
    "    'title',\n",
    "    'abstract',\n",
    "    'venue',\n",
    "    'title_emb',\n",
    "    'abstract_emb',\n",
    "    'venue_emb',\n",
    "    'true_author',\n",
    "]\n",
    "\n",
    "gs = GraphSampling(\n",
    "    node_spec=node_spec,\n",
    "    edge_spec=edge_spec,\n",
    "    node_properties=node_properties,\n",
    "    database='homogeneous-graph',\n",
    ")\n",
    "\n",
    "# Model configurations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = {\n",
    "    'experiment': 'GATv2 encoder trained on graph (publication nodes, similarity and co-author edges) using Pairwise Contrastive Loss',\n",
    "    'max_hops': 2,\n",
    "    'model_node_feature': 'abstract_emb',  # Node feature to use for GAT encoder\n",
    "    'hidden_channels': 128,\n",
    "    'out_channels': 32,\n",
    "    'num_heads': 8,\n",
    "    'margin': 1.0,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 0.005,\n",
    "    'weight_decay': 5e-4,\n",
    "    'num_epochs': 5,\n",
    "}"
   ],
   "id": "91b04efb689d61f4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph Pair Data Handling",
   "id": "116d0a70795de1aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.738960Z",
     "start_time": "2024-10-13T10:18:54.734964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraphPairDataset(Dataset):\n",
    "    def __init__(self, pairs, gs):\n",
    "        self.pairs = pairs  # List of tuples: (paper_id1, paper_id2, label)\n",
    "        self.gs = gs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paper_id1, paper_id2, label = self.pairs[idx]\n",
    "        try:\n",
    "            #print(f\"Processing pair ({paper_id1}, {paper_id2})\")\n",
    "            # Get n-hop neighbourhood for each paper\n",
    "            graph1 = self.gs.n_hop_neighbourhood(NodeType.PUBLICATION, paper_id1, max_level=config['max_hops'])\n",
    "            graph2 = self.gs.n_hop_neighbourhood(NodeType.PUBLICATION, paper_id2, max_level=config['max_hops'])\n",
    "\n",
    "            # Convert to PyG Data objects\n",
    "            data1, node_map_1 = neo_to_pyg_hetero_edges(graph1, config['model_node_feature'])\n",
    "            data1.central_node_id = torch.tensor([node_map_1[paper_id1]])\n",
    "            \n",
    "            data2, node_map_2 = neo_to_pyg_hetero_edges(graph2, config['model_node_feature'])\n",
    "            data2.central_node_id = torch.tensor([node_map_2[paper_id2]])\n",
    "            \n",
    "            # Return data and label\n",
    "            return data1, data2, torch.tensor(label, dtype=torch.float)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair ({paper_id1}, {paper_id2}): {e}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "# This is required for the PyG DataLoader in order to handle the custom mini-batching during training \n",
    "class PairData(HeteroData):\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "        if key == 'central_node_id':\n",
    "            return 0  # Concat along batch dim\n",
    "        else:\n",
    "            return super().__cat_dim__(key, value, *args, **kwargs)\n",
    "        \n",
    "    def __inc__(self, key: str, value: Any, *args, **kwargs) -> Any:\n",
    "        if key == 'central_node_id':\n",
    "            return self.num_nodes\n",
    "        else:\n",
    "            return super().__inc__(key, value, *args, **kwargs)"
   ],
   "id": "6fed7b3a506508a0",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.782853Z",
     "start_time": "2024-10-13T10:18:54.779947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graph_data_valid(data: Data):\n",
    "    try:\n",
    "        node_type_val = NodeType.PUBLICATION.value\n",
    "        assert data is not None, \"Data object is None.\"\n",
    "        assert data.num_nodes > 0, \"Number of nodes must be greater than 0.\"\n",
    "        assert data.num_edges > 0, \"Number of edges must be greater than 0.\"\n",
    "        assert data[node_type_val].x is not None, \"Node features 'x' are missing.\"\n",
    "        assert data[node_type_val].x.size(0) == data.num_nodes, \"Mismatch between 'x' size and 'num_nodes'.\"\n",
    "        assert data[node_type_val].x.dtype in (torch.float32, torch.float64), \"Node features 'x' must be floating point.\"\n",
    "        for key in [edge_pyg_key_vals[r] for r in edge_spec]:\n",
    "            if key not in data:\n",
    "                continue\n",
    "            assert data[key].edge_index.size(0) == 2, f\"'edge_index' for '{key}' should have shape [2, num_edges].\"\n",
    "            assert data[key].edge_index.size(1) == data[key].num_edges, f\"Mismatch between 'edge_index' and 'num_edges' for '{key}'.\"\n",
    "            assert data[key].edge_index is not None, f\"Edge index for '{key}' is missing.\"\n",
    "            assert data[key].edge_index.max() < data.num_nodes, f\"'edge_index' for '{key}' contains invalid node indices.\"\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"Data check failed: {e}\")\n",
    "        return False"
   ],
   "id": "e5c36b0c09aa6625",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.830324Z",
     "start_time": "2024-10-13T10:18:54.825822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def neo_to_pyg_hetero_edges(\n",
    "        data,\n",
    "        node_attr: str,\n",
    "):\n",
    "    if not data:\n",
    "        return None, None\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    relationships = data[\"relationships\"]\n",
    "\n",
    "    #print(f\"Nodes: {len(nodes)}, Relationships: {len(relationships)}\")\n",
    "\n",
    "    # Create a PyG Data object\n",
    "    pyg_data = PairData()\n",
    "\n",
    "    node_features = []\n",
    "    node_ids = []\n",
    "    node_id_map = {}\n",
    "\n",
    "    for node in nodes:\n",
    "        node_id = node.get(\"id\")\n",
    "        node_feature = node.get(node_attr, None)\n",
    "        if node_feature is None:\n",
    "            print(f\"Node {node_id} has no attribute {node_attr}\")\n",
    "            continue\n",
    "\n",
    "        # Map node id to its index in the list\n",
    "        idx = len(node_ids)\n",
    "        node_id_map[node_id] = idx\n",
    "        node_ids.append(node_id)\n",
    "\n",
    "        # Convert node features to tensors\n",
    "        node_feature_tensor = torch.tensor(node_feature, dtype=torch.float32)\n",
    "        node_features.append(node_feature_tensor)\n",
    "\n",
    "    # Convert list of features to tensor\n",
    "    if node_features:\n",
    "        pyg_data[NodeType.PUBLICATION.value].x = torch.vstack(node_features)\n",
    "        pyg_data[NodeType.PUBLICATION.value].num_nodes = pyg_data[NodeType.PUBLICATION.value].x.size(0)\n",
    "    else:\n",
    "        print(\"No node features available.\")\n",
    "        return None, None\n",
    "\n",
    "    # Process relationships\n",
    "    edge_dict = {}\n",
    "\n",
    "    for rel in relationships:\n",
    "        key = edge_val_to_pyg_key_vals[rel.type]\n",
    "        if key not in edge_dict:\n",
    "            edge_dict[key] = [[], []]\n",
    "\n",
    "        source_id = rel.start_node.get(\"id\")\n",
    "        target_id = rel.end_node.get(\"id\")\n",
    "\n",
    "        # Append the indices of the source and target nodes\n",
    "        edge_dict[key][0].append(node_id_map[source_id])\n",
    "        edge_dict[key][1].append(node_id_map[target_id])\n",
    "\n",
    "    # Convert edge lists to tensors\n",
    "    for key in edge_dict:\n",
    "        pyg_data[key[0], key[1], key[2]].edge_index = torch.vstack([\n",
    "            torch.tensor(edge_dict[key][0], dtype=torch.long),\n",
    "            torch.tensor(edge_dict[key][1], dtype=torch.long)\n",
    "        ])\n",
    "\n",
    "        pyg_data[key[0], key[1], key[2]].edge_attr = torch.vstack(\n",
    "            [edge_one_hot[key[1]] for _ in range(len(edge_dict[key][0]))])\n",
    "\n",
    "    return pyg_data, node_id_map"
   ],
   "id": "9f549c0dc7623d66",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Harvest Positive and Negative tuples from the graph database",
   "id": "b9eda2497b302be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.878911Z",
     "start_time": "2024-10-13T10:18:54.872772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataHarvester:\n",
    "    def __init__(self, db: DatabaseWrapper, gs: GraphSampling):\n",
    "        self.db = db\n",
    "        self.gs = gs\n",
    "        self.pairs = []\n",
    "        self.prepare_pairs()\n",
    "\n",
    "    def prepare_pairs(self):\n",
    "        print(\"Preparing pairs...\")\n",
    "        file_path = './data/hetero-pairs.json'\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading pairs...\")\n",
    "            self.load_pairs(file_path)\n",
    "            print(f\"Loaded {len(self.pairs)} pairs.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Could not load pairs from file. Generating pairs...\")\n",
    "            self.generate_pairs()\n",
    "            print(f\"Generated {len(self.pairs)} pairs.\")\n",
    "            print(\"Saving pairs...\")\n",
    "            self.save_pairs(file_path)\n",
    "            print(\"Pairs saved.\")\n",
    "            \n",
    "    def load_pairs(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.pairs = json.load(f)\n",
    "    \n",
    "    def save_pairs(self, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.pairs, f)\n",
    "                \n",
    "    def generate_pairs(self):\n",
    "        # Filter out the papers that are not present in the graph or have less than 2 edges\n",
    "        paper_ids = []\n",
    "        print(\"Checking data validity...\")\n",
    "        for nodes in self.db.iter_nodes_with_edge_count(NodeType.PUBLICATION, edge_spec, ['id', 'true_author']):\n",
    "            for node in nodes:\n",
    "                data = gs.n_hop_neighbourhood(NodeType.PUBLICATION, node['id'], max_level=1)\n",
    "                data = neo_to_pyg_hetero_edges(data, config['model_node_feature'])[0]\n",
    "                if not graph_data_valid(data):\n",
    "                    continue\n",
    "                paper_ids.append(node['id'])\n",
    "        \n",
    "        print(f\"Total papers: {len(paper_ids)}\")\n",
    "        print(\"Preparing pairs...\")\n",
    "        paper_set = set(paper_ids)\n",
    "        \n",
    "        author_data = WhoIsWhoDataset.parse_train()\n",
    "        for author_id, data in author_data.items():\n",
    "            for key in data:\n",
    "                data[key] = [p_id for p_id in data[key] if p_id in paper_set]\n",
    "        \n",
    "        # Generate pairs with labels\n",
    "        pairs = []\n",
    "        \n",
    "        for author_id, data in author_data.items():\n",
    "            normal_data = data.get('normal_data', [])\n",
    "            outliers = data.get('outliers', [])\n",
    "                    \n",
    "            # Positive pairs: combinations of normal_data\n",
    "            pos_pairs = list(combinations(normal_data, 2))\n",
    "            if len(pos_pairs) > 50:\n",
    "                pos_pairs = random.sample(pos_pairs, 50)\n",
    "            for pair in pos_pairs:\n",
    "                pairs.append((pair[0], pair[1], 1))\n",
    "            \n",
    "            # Negative pairs: product of normal_data and outliers\n",
    "            neg_pairs = list(product(normal_data, outliers))\n",
    "            if len(neg_pairs) > 50:\n",
    "                neg_pairs = random.sample(neg_pairs, 50)\n",
    "            elif len(neg_pairs) < len(pos_pairs):\n",
    "                # Sample random paper ids from other authors\n",
    "                while len(neg_pairs) < len(pos_pairs):\n",
    "                    p1 = random.choice(normal_data)\n",
    "                    p2 = random.choice(paper_ids)\n",
    "                    if p2 not in normal_data:\n",
    "                        neg_pairs.append((p1, p2))\n",
    "            for pair in neg_pairs:\n",
    "                pairs.append((pair[0], pair[1], 0))\n",
    "        \n",
    "        print(f\"Total pairs: {len(pairs)}. Done.\")\n",
    "        self.pairs = pairs\n",
    "                "
   ],
   "id": "5b8e23ac5ebacd35",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GAT Encoder",
   "id": "b1431a2c7d9c9e59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.923597Z",
     "start_time": "2024-10-13T10:18:54.920811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class HeteroGATEncoder(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: GATv2Conv(\n",
    "                (-1, -1), hidden_channels, heads=num_heads, concat=True)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: GATv2Conv(\n",
    "                (-1, -1), out_channels, heads=1, concat=False)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict = data.x_dict\n",
    "        edge_index_dict = data.edge_index_dict\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.elu(x) for key, x in x_dict.items()}\n",
    "\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict"
   ],
   "id": "c0ebc105a776068a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:54.967856Z",
     "start_time": "2024-10-13T10:18:54.965924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def contrastive_loss(embeddings1, embeddings2, labels, margin=1.0):\n",
    "    # Compute Euclidean distances between embeddings\n",
    "    distances = F.pairwise_distance(embeddings1, embeddings2)\n",
    "    \n",
    "    # Loss\n",
    "    loss_pos = labels * distances.pow(2)  # For positive pairs\n",
    "    loss_neg = (1 - labels) * F.relu(margin - distances).pow(2)  # For negative pairs\n",
    "    loss = loss_pos + loss_neg\n",
    "    return loss.mean(), distances\n"
   ],
   "id": "1860d722d7a121d1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:55.011211Z",
     "start_time": "2024-10-13T10:18:55.009003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# custom collate function adjusted for GraphPairDataset\n",
    "def custom_collate(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None  # Skip empty batches\n",
    "\n",
    "    data1_list = [item[0] for item in batch]\n",
    "    data2_list = [item[1] for item in batch]\n",
    "    \n",
    "    labels = torch.stack([item[2] for item in batch])\n",
    "    \n",
    "    batch1 = Batch.from_data_list(data1_list)\n",
    "    batch2 = Batch.from_data_list(data2_list)\n",
    "\n",
    "    return batch1, batch2, labels"
   ],
   "id": "1529ad8e8ee337ce",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:55.056159Z",
     "start_time": "2024-10-13T10:18:55.052976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot loss\n",
    "def plot_loss(losses, epoch_marker_pos=None, plot_title=\"Loss\", plot_file=None):\n",
    "    if plot_file is None:\n",
    "        plot_file = f'./data/losses/loss.png'\n",
    "    if epoch_marker_pos is None:\n",
    "        epoch_marker_pos = []\n",
    "        \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=f'Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    for ix, x_pos in enumerate(epoch_marker_pos):\n",
    "        plt.axvline(x=x_pos, color='red', linestyle='dotted', linewidth=1)\n",
    "        plt.text(\n",
    "        x_pos,\n",
    "        max(losses),\n",
    "        f'Epoch {ix}',\n",
    "        rotation=90,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        fontsize=10,\n",
    "        color='red'\n",
    "    )\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    \n",
    "def save_training_results(train_loss, test_loss, eval_results, config, file_path):\n",
    "    results = {\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'eval_results': eval_results,\n",
    "        'config': config,\n",
    "    }\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ],
   "id": "76a48fbe61da1a81",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:55.102069Z",
     "start_time": "2024-10-13T10:18:55.097897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, batch1, batch2, labels, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch1 = batch1.to(device)\n",
    "    batch2 = batch2.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    embeddings1 = model(batch1)\n",
    "    embeddings2 = model(batch2)\n",
    "\n",
    "    embeddings1_central = embeddings1[NodeType.PUBLICATION.value][batch1.central_node_id]\n",
    "    embeddings2_central = embeddings2[NodeType.PUBLICATION.value][batch2.central_node_id]\n",
    "\n",
    "    loss, _ = contrastive_loss(embeddings1_central, embeddings2_central, labels, config['margin'])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    batch_loss = loss.item()\n",
    "    #print(f\"Batch loss: {batch_loss:.4f}\")\n",
    "    return batch_loss\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    distances = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch1, batch2, labels in dataloader:\n",
    "            batch1 = batch1.to(device)\n",
    "            batch2 = batch2.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            embeddings1 = model(batch1)\n",
    "            embeddings2 = model(batch2)\n",
    "            \n",
    "            embeddings1_central = embeddings1[NodeType.PUBLICATION.value][batch1.central_node_id]\n",
    "            embeddings2_central = embeddings2[NodeType.PUBLICATION.value][batch2.central_node_id]\n",
    "            \n",
    "            loss, dist = contrastive_loss(embeddings1_central, embeddings2_central, labels, config['margin'])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            distances.extend(dist.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "        \n",
    "    # Compute accuracy\n",
    "    distances = np.array(distances)\n",
    "    labels_list = np.array(labels_list).astype(int)\n",
    "    predictions = (distances <= config['margin']).astype(int)\n",
    "    accuracy = accuracy_score(labels_list, predictions)\n",
    "    \n",
    "    # Compute average loss    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "    "
   ],
   "id": "2ff2cd9e6fd091ef",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training ",
   "id": "b67cf89d3d17cc12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T10:18:55.174471Z",
     "start_time": "2024-10-13T10:18:55.143960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DatabaseWrapper(database='homogeneous-graph')\n",
    "data_harvester = DataHarvester(db=db, gs=gs)\n",
    "\n",
    "\n",
    "# Split the pairs into train and test\n",
    "random.shuffle(data_harvester.pairs)\n",
    "train_size = int(0.95 * len(data_harvester.pairs))\n",
    "train_pairs = data_harvester.pairs[:train_size]\n",
    "test_pairs = data_harvester.pairs[train_size:]\n",
    "config['train_size'] = train_size\n",
    "config['test_size'] = len(data_harvester.pairs) - train_size\n",
    "\n",
    "# Create the datasets from the pairs (distinct pairs for training and testing)\n",
    "train_dataset = GraphPairDataset(train_pairs, gs)\n",
    "test_dataset = GraphPairDataset(test_pairs, gs)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 32\n",
    "config['batch_size'] = batch_size\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# Create model\n",
    "metadata = (\n",
    "    [n.value for n in node_spec],\n",
    "    [edge_pyg_key_vals[r] for r in edge_spec]\n",
    ")\n",
    "config['node_spec'] = metadata[0]\n",
    "config['edge_spec'] = metadata[1]\n",
    "model = HeteroGATEncoder(metadata, config['hidden_channels'], config['out_channels'], num_heads=config['num_heads']).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])"
   ],
   "id": "62ef1028a5c5fd5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 12:18:55,147 - DatabaseWrapper - INFO - Connecting to the database ...\n",
      "2024-10-13 12:18:55,147 - DatabaseWrapper - INFO - Database ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing pairs...\n",
      "Loading pairs...\n",
      "Loaded 6308 pairs.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:49:59.315524Z",
     "start_time": "2024-10-13T10:18:55.193107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = config['num_epochs']\n",
    "margin = 1.0  # Margin for contrastive loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_marker_pos = list(range(0, len(train_dataloader) * epoch, len(train_dataloader)))\n",
    "    \n",
    "    for data1, data2, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        if data1 is None or data2 is None:\n",
    "            continue\n",
    "        \n",
    "        if len(train_losses) % 10 == 0:\n",
    "            test_loss, test_accuracy = test(model, test_dataloader)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_epoch_marker_pos = [marker/10 for marker in epoch_marker_pos if marker != 0]\n",
    "            plot_loss(test_losses, epoch_marker_pos=test_epoch_marker_pos, plot_title='Test Loss', plot_file=f'./data/losses/test_loss_hetero_edges.png')\n",
    "            plot_loss(test_accuracies, epoch_marker_pos=test_epoch_marker_pos, plot_title='Test Accuracy', plot_file=f'./data/losses/test_accuracy_hetero_edges.png')\n",
    "            \n",
    "        loss = train(model, data1, data2, labels, optimizer)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        plot_loss(train_losses, epoch_marker_pos=epoch_marker_pos, plot_title='Training Loss', plot_file=f'./data/losses/train_loss_hetero_edges.png')\n",
    "    \n",
    "    # Save config and training results\n",
    "    save_training_results(train_losses, test_losses, None, config, f'./data/results/training_results_hetero_edges.json')\n",
    "    \n",
    "    # Save model if loss has decreased\n",
    "    if len(test_losses) > 1 and test_losses[-1] < min(test_losses[:-1]):\n",
    "        print(f\"Saving model at epoch {epoch}...\")\n",
    "        torch.save(model.state_dict(), f'./data/models/gat_encoder_hetero_edges.pt')"
   ],
   "id": "c3b28cd52881796",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/188 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "628852f4a8a544e3bcc8331b1a38d679"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7251, Test Accuracy: 0.5190\n",
      "Test Loss: 9.3339, Test Accuracy: 0.4937\n",
      "Test Loss: 10.0982, Test Accuracy: 0.4905\n",
      "Test Loss: 6.3531, Test Accuracy: 0.4937\n",
      "Test Loss: 4.9800, Test Accuracy: 0.4937\n",
      "Test Loss: 3.4738, Test Accuracy: 0.4905\n",
      "Test Loss: 3.1983, Test Accuracy: 0.4905\n",
      "Test Loss: 1.9555, Test Accuracy: 0.4905\n",
      "Test Loss: 1.5979, Test Accuracy: 0.4968\n",
      "Test Loss: 1.7625, Test Accuracy: 0.5095\n",
      "Test Loss: 1.1818, Test Accuracy: 0.4842\n",
      "Test Loss: 1.0938, Test Accuracy: 0.5285\n",
      "Test Loss: 0.8690, Test Accuracy: 0.5222\n",
      "Test Loss: 0.7008, Test Accuracy: 0.5095\n",
      "Test Loss: 0.6986, Test Accuracy: 0.5095\n",
      "Test Loss: 0.5995, Test Accuracy: 0.4905\n",
      "Test Loss: 0.5819, Test Accuracy: 0.5380\n",
      "Test Loss: 0.5540, Test Accuracy: 0.5475\n",
      "Test Loss: 0.5174, Test Accuracy: 0.5032\n",
      "Saving model at epoch 1...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/188 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abfef315b9c04d12b126de0fafacd01b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4558, Test Accuracy: 0.5158\n",
      "Test Loss: 0.4119, Test Accuracy: 0.5285\n",
      "Test Loss: 0.3697, Test Accuracy: 0.5316\n",
      "Test Loss: 0.3957, Test Accuracy: 0.5570\n",
      "Test Loss: 0.3494, Test Accuracy: 0.5411\n",
      "Test Loss: 0.3332, Test Accuracy: 0.5127\n",
      "Test Loss: 0.3211, Test Accuracy: 0.5095\n",
      "Test Loss: 0.3388, Test Accuracy: 0.5348\n",
      "Test Loss: 0.3266, Test Accuracy: 0.5063\n",
      "Test Loss: 0.3225, Test Accuracy: 0.5411\n",
      "Test Loss: 0.3020, Test Accuracy: 0.5222\n",
      "Test Loss: 0.3082, Test Accuracy: 0.5253\n",
      "Test Loss: 0.2852, Test Accuracy: 0.5095\n",
      "Test Loss: 0.2947, Test Accuracy: 0.5063\n",
      "Test Loss: 0.2996, Test Accuracy: 0.5348\n",
      "Test Loss: 0.3336, Test Accuracy: 0.5316\n",
      "Test Loss: 0.3486, Test Accuracy: 0.5158\n",
      "Test Loss: 0.3105, Test Accuracy: 0.5316\n",
      "Test Loss: 0.3127, Test Accuracy: 0.5063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/188 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67ce83c04e194139b502a5f6186ad481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3147, Test Accuracy: 0.5127\n",
      "Test Loss: 0.2967, Test Accuracy: 0.5222\n",
      "Test Loss: 0.2954, Test Accuracy: 0.5158\n",
      "Test Loss: 0.2859, Test Accuracy: 0.5222\n",
      "Test Loss: 0.2797, Test Accuracy: 0.5222\n",
      "Test Loss: 0.3085, Test Accuracy: 0.5190\n",
      "Test Loss: 0.3040, Test Accuracy: 0.5316\n",
      "Test Loss: 0.3363, Test Accuracy: 0.5348\n",
      "Test Loss: 0.2772, Test Accuracy: 0.5127\n",
      "Test Loss: 0.3070, Test Accuracy: 0.5380\n",
      "Test Loss: 0.2937, Test Accuracy: 0.5348\n",
      "Test Loss: 0.2941, Test Accuracy: 0.5222\n",
      "Test Loss: 0.2558, Test Accuracy: 0.5127\n",
      "Test Loss: 0.2749, Test Accuracy: 0.5222\n",
      "Test Loss: 0.3039, Test Accuracy: 0.5411\n",
      "Test Loss: 0.2807, Test Accuracy: 0.5190\n",
      "Test Loss: 0.2795, Test Accuracy: 0.5222\n",
      "Test Loss: 0.2690, Test Accuracy: 0.5095\n",
      "Test Loss: 0.2752, Test Accuracy: 0.5190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/188 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67c8a65bbc9f4bc18eece22911e3fbff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3052, Test Accuracy: 0.5158\n",
      "Test Loss: 0.3002, Test Accuracy: 0.5222\n",
      "Test Loss: 0.3297, Test Accuracy: 0.5380\n",
      "Test Loss: 0.3688, Test Accuracy: 0.4937\n",
      "Test Loss: 0.2641, Test Accuracy: 0.5000\n",
      "Test Loss: 0.3141, Test Accuracy: 0.5316\n",
      "Test Loss: 0.3519, Test Accuracy: 0.5316\n",
      "Test Loss: 0.2671, Test Accuracy: 0.5285\n",
      "Test Loss: 0.2619, Test Accuracy: 0.5285\n",
      "Test Loss: 0.2572, Test Accuracy: 0.5285\n",
      "Test Loss: 0.2551, Test Accuracy: 0.5348\n",
      "Test Loss: 0.2849, Test Accuracy: 0.5411\n",
      "Test Loss: 0.2720, Test Accuracy: 0.5285\n",
      "Test Loss: 0.2895, Test Accuracy: 0.5222\n",
      "Test Loss: 0.2771, Test Accuracy: 0.5063\n",
      "Test Loss: 0.2683, Test Accuracy: 0.5032\n",
      "Test Loss: 0.2842, Test Accuracy: 0.5158\n",
      "Test Loss: 0.2589, Test Accuracy: 0.5032\n",
      "Test Loss: 0.2677, Test Accuracy: 0.5158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/188 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be7d87e47cb4a8e8b21cd988f586ae3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2700, Test Accuracy: 0.5222\n",
      "Test Loss: 0.2864, Test Accuracy: 0.5348\n",
      "Test Loss: 0.2776, Test Accuracy: 0.5506\n",
      "Test Loss: 0.2528, Test Accuracy: 0.5285\n",
      "Test Loss: 0.2785, Test Accuracy: 0.5411\n",
      "Test Loss: 0.2705, Test Accuracy: 0.5190\n",
      "Test Loss: 0.2642, Test Accuracy: 0.5411\n",
      "Test Loss: 0.2606, Test Accuracy: 0.5253\n",
      "Test Loss: 0.2521, Test Accuracy: 0.5158\n",
      "Test Loss: 0.2707, Test Accuracy: 0.5190\n",
      "Test Loss: 0.2869, Test Accuracy: 0.5190\n",
      "Test Loss: 0.2629, Test Accuracy: 0.5316\n",
      "Test Loss: 0.2410, Test Accuracy: 0.5158\n",
      "Test Loss: 0.2599, Test Accuracy: 0.5063\n",
      "Test Loss: 0.2650, Test Accuracy: 0.5348\n",
      "Test Loss: 0.2596, Test Accuracy: 0.5222\n",
      "Test Loss: 0.3143, Test Accuracy: 0.5253\n",
      "Test Loss: 0.2565, Test Accuracy: 0.5222\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:49:59.346907Z",
     "start_time": "2024-10-13T12:49:59.331213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    distances = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch1, batch2, labels in dataloader:\n",
    "            batch1 = batch1.to(device)\n",
    "            batch2 = batch2.to(device)\n",
    "            embeddings1 = model(batch1)\n",
    "            embeddings2 = model(batch2)\n",
    "            dist = F.pairwise_distance(embeddings1, embeddings2).cpu().numpy()\n",
    "            distances.extend(dist)\n",
    "            labels_list.extend(labels.numpy())\n",
    "    return distances, labels_list\n",
    "\n",
    "# After training\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "distances, labels_list = evaluate(model, dataloader)\n",
    "roc_auc = roc_auc_score(labels_list, -np.array(distances))  # Negative distances for similarity\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ],
   "id": "1955b297484a6fa3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# After training\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m roc_auc_score\n\u001B[0;32m---> 20\u001B[0m distances, labels_list \u001B[38;5;241m=\u001B[39m evaluate(model, \u001B[43mdataloader\u001B[49m)\n\u001B[1;32m     21\u001B[0m roc_auc \u001B[38;5;241m=\u001B[39m roc_auc_score(labels_list, \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39marray(distances))  \u001B[38;5;66;03m# Negative distances for similarity\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mROC AUC Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mroc_auc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
