{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sympy.tensor.indexed import IndexException\n",
    "from tqdm.notebook import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from src.datasets.who_is_who import WhoIsWhoDataset\n",
    "from src.shared.graph_schema import NodeType, EdgeType, node_one_hot, edge_one_hot, edge_val_to_pyg_key_vals\n",
    "from src.shared import config\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "driver = GraphDatabase.driver(config.DB_URI, auth=(config.DB_USER, config.DB_PASSWORD))",
   "id": "3a6f87824453abb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fetch data from Neo4j and create PyG HeteroData objects",
   "id": "fc0b0b92ec8a6f7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "included_nodes = [\n",
    "    NodeType.PUBLICATION, \n",
    "    NodeType.VENUE, \n",
    "    NodeType.ORGANIZATION,\n",
    "    NodeType.AUTHOR,\n",
    "    NodeType.CO_AUTHOR\n",
    "]\n",
    "included_edges = [\n",
    "    EdgeType.PUB_VENUE,\n",
    "    EdgeType.VENUE_PUB,\n",
    "    EdgeType.PUB_ORG,\n",
    "    EdgeType.ORG_PUB, \n",
    "    EdgeType.PUB_AUTHOR,\n",
    "    EdgeType.AUTHOR_PUB,\n",
    "    EdgeType.AUTHOR_ORG,\n",
    "    EdgeType.ORG_AUTHOR,\n",
    "    EdgeType.PUB_ORG,\n",
    "    EdgeType.ORG_PUB,\n",
    "]\n",
    "\n",
    "def verify_hetero_data(h_data: HeteroData) -> bool:\n",
    "    # Check if there are at least 2 nodes in total\n",
    "    total_nodes = 0\n",
    "    for node_type in h_data.node_types:\n",
    "        if 'x' in h_data[node_type]:\n",
    "            total_nodes += h_data[node_type].x.size(0)\n",
    "\n",
    "    if total_nodes < 2:\n",
    "        print(f\"Error: The HeteroData object should contain at least 2 nodes, but contains {total_nodes}.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "    \n",
    "def fetch_n_hop_neighbourhood(\n",
    "        start_node_type: NodeType, \n",
    "        start_node_id: str,\n",
    "        node_attr: str, \n",
    "        node_types: list = None, \n",
    "        edge_types: list = None,\n",
    "        max_level: int = 6\n",
    "):\n",
    "    with driver.session() as session:\n",
    "        node_filter = '|'.join(\n",
    "            [nt.value for nt in NodeType] if node_types is None else \n",
    "            [nt.value for nt in node_types]\n",
    "        )\n",
    "        edge_filter = '|'.join(\n",
    "            [f\"<{et.value}\" for et in EdgeType] if edge_types is None else \n",
    "            [f\"<{et.value}\" for et in edge_types]\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "                MATCH (start:{start_node_type.value} {{id: '{start_node_id}'}})\n",
    "                CALL apoc.path.subgraphAll(start, {{\n",
    "                  maxLevel: {max_level},\n",
    "                  relationshipFilter: '{edge_filter}',\n",
    "                  labelFilter: '+{node_filter}'\n",
    "                }}) YIELD nodes, relationships\n",
    "                RETURN nodes, relationships\n",
    "            \"\"\"\n",
    "        result = session.run(query)\n",
    "        data = result.single()\n",
    "        if not data:\n",
    "            return None, None\n",
    "        \n",
    "        nodes = data[\"nodes\"]\n",
    "        relationships = data[\"relationships\"]\n",
    "        print(f\"Start node id: {start_node_id}\")\n",
    "        print(f\"Nodes: {len(nodes)}, Relationships: {len(relationships)}\")\n",
    "        if len(nodes) > 500:\n",
    "            print(f\"Too many nodes: {len(nodes)}\")\n",
    "            return None, None\n",
    "    \n",
    "        # Create data object\n",
    "        h_data = HeteroData()\n",
    "        \n",
    "        node_features = {}\n",
    "        node_ids = {}\n",
    "        node_id_map = {}\n",
    "        \n",
    "        for node in nodes:\n",
    "            node_id = node.get(\"id\")\n",
    "            node_feature = node.get(node_attr, None)\n",
    "            if node_feature is None:\n",
    "                print(f\"Node {node_id} has no attribute {node_attr}\")\n",
    "                continue\n",
    "            node_label = list(node.labels)[0]\n",
    "            if node_label not in node_features:\n",
    "                node_features[node_label] = []\n",
    "                node_ids[node_label] = []\n",
    "            \n",
    "            # Convert node features to tensors\n",
    "            node_features[node_label].append(torch.tensor(node_feature, dtype=torch.float32))\n",
    "            node_ids[node_label].append(node_id)\n",
    "            \n",
    "            # Map node ID to its index in the list\n",
    "            node_id_map[node_id] = len(node_ids[node_label]) - 1\n",
    "        \n",
    "        # Convert list of features to a single tensor per node type\n",
    "        for node_label, node_features in node_features.items():\n",
    "            h_data[node_label].x = torch.vstack(node_features)\n",
    "            #print(f\"Node {node_label} x: {h_data[node_label].x.shape}\")\n",
    "        \n",
    "        # Process relationships\n",
    "        edge_dict = {}\n",
    "        \n",
    "        for rel in relationships:\n",
    "            key = edge_val_to_pyg_key_vals[rel.type]  # edge_val_to_pyg_key_vals maps edge types to tuples (src, dst)\n",
    "            if key not in edge_dict:\n",
    "                edge_dict[key] = [[], []]\n",
    "                \n",
    "            source_id = rel.start_node.get(\"id\")\n",
    "            target_id = rel.end_node.get(\"id\")\n",
    "            \n",
    "            # Append the indices of the source and target nodes\n",
    "            edge_dict[key][0].append(node_id_map[source_id])\n",
    "            edge_dict[key][1].append(node_id_map[target_id])\n",
    "        \n",
    "        # Convert edge lists to tensors\n",
    "        for key in edge_dict:\n",
    "            h_data[key[0], key[1], key[2]].edge_index = torch.vstack([\n",
    "                torch.tensor(edge_dict[key][0], dtype=torch.long),\n",
    "                torch.tensor(edge_dict[key][1], dtype=torch.long)\n",
    "            ])\n",
    "            #print(f\"Edge index: {h_data[key[0], key[1], key[2]].edge_index.shape}\")\n",
    "            h_data[key[0], key[1], key[2]].edge_attr = torch.vstack([edge_one_hot[key[1]] for _ in range(len(edge_dict[key][0]))])\n",
    "            #print(f\"Edge attr: {h_data[key[0], key[1], key[2]].edge_attr.shape}\")\n",
    "    \n",
    "    if not verify_hetero_data(h_data):\n",
    "        return None, None\n",
    "    \n",
    "    return h_data, node_id_map\n",
    "\n",
    "def sample_triplet(anchor, pos, neg):\n",
    "    triplet = {}\n",
    "    for label, node_id in zip([\"anchor\", \"pos\", \"neg\"], [anchor, pos, neg]):\n",
    "        h_data, node_id_map = fetch_n_hop_neighbourhood(\n",
    "            start_node_type=NodeType.PUBLICATION, \n",
    "            start_node_id=node_id, \n",
    "            node_attr=\"vec\",\n",
    "            node_types=included_nodes,\n",
    "            edge_types=included_edges,\n",
    "            max_level=2\n",
    "        )\n",
    "        if not h_data or not node_id_map:\n",
    "            return None\n",
    "        \n",
    "        graph = {\n",
    "            \"data\": h_data,\n",
    "            \"node_id_map\": node_id_map,\n",
    "            \"pub_node_id\": node_id\n",
    "        }\n",
    "        \n",
    "        triplet[label] = graph\n",
    "    \n",
    "    return triplet\n",
    "\n",
    "\n",
    "def sample_graphs(start_node_ids: list):\n",
    "    graphs = []\n",
    "    node_id_maps = []\n",
    "\n",
    "    for node_id in start_node_ids:\n",
    "        h_data, node_id_map = fetch_n_hop_neighbourhood(\n",
    "            start_node_type=NodeType.PUBLICATION, \n",
    "            start_node_id=node_id, \n",
    "            node_attr=\"vec\",\n",
    "            node_types=included_nodes,\n",
    "            edge_types=included_edges,\n",
    "            max_level=2\n",
    "        )\n",
    "        if not h_data or not node_id_map:\n",
    "            continue\n",
    "        \n",
    "        graphs.append(h_data)\n",
    "        node_id_maps.append(node_id_map)\n",
    "    \n",
    "    return graphs, node_id_maps"
   ],
   "id": "839799248565909e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TripletSampler:\n",
    "    def __init__(self):\n",
    "        self.data = WhoIsWhoDataset.parse_train()\n",
    "        self.unsampled_authors = list(self.data.keys())\n",
    "            \n",
    "    def sample_triplet_ids(self, attempt = 0):\n",
    "        if attempt > 30:\n",
    "            print(\"Unable to sample more triples\")\n",
    "            return None\n",
    "        # Get random author\n",
    "        author_id = random.choice(list(self.data.keys()))\n",
    "        \n",
    "        # Check if author has enough data\n",
    "        normal_data = self.data[author_id][\"normal_data\"]\n",
    "        outliers = self.data[author_id][\"outliers\"]\n",
    "        if len(normal_data) < 2 or len(outliers) == 0:\n",
    "            self.data.pop(author_id)\n",
    "            return self.sample_triplet_ids(attempt + 1)\n",
    "        # Get random anchor, positive and negative samples, remove anchor\n",
    "        anchor_id = random.choice(normal_data)\n",
    "        self.data[author_id][\"normal_data\"].remove(anchor_id)\n",
    "        \n",
    "        pos_sample = random.choice(normal_data)\n",
    "        neg_sample = random.choice(outliers)\n",
    "        return anchor_id, pos_sample, neg_sample\n",
    "\n",
    "    def iter_triplets_rand(self, num_triplets):\n",
    "        # Fetch triplets by id from the neo4j database and yield them\n",
    "        count = 0\n",
    "        count_skipped = 0\n",
    "        while count < num_triplets:                \n",
    "            anchor_id, pos_id, neg_id = self.sample_triplet_ids()\n",
    "            triplet = sample_triplet(anchor_id, pos_id, neg_id)\n",
    "            if triplet is None or type(triplet) is not dict:\n",
    "                count_skipped += 1\n",
    "                continue\n",
    "        \n",
    "            yield triplet\n",
    "            count += 1\n",
    "            \n",
    "    def sample_author(self):\n",
    "        try:\n",
    "            author_id = self.unsampled_authors[0]\n",
    "            self.unsampled_authors.remove(author_id)\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "            return None, None, None\n",
    "        normal_data = self.data[author_id][\"normal_data\"]\n",
    "        outliers = self.data[author_id][\"outliers\"]\n",
    "        \n",
    "        return author_id, normal_data, outliers"
   ],
   "id": "77a1b83b28930def",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "triplet_sampler = TripletSampler()\n",
    "path = \"./data/triplet_dataset\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Delete folder contents\n",
    "for file in os.listdir(path):\n",
    "    os.remove(os.path.join(path, file))\n",
    "\n",
    "with tqdm(total=10000) as pbar:\n",
    "    for batch_id in range(10000):\n",
    "        file = os.path.join(path, f\"triplet_batch_{batch_id}.pt\")\n",
    "        #with open(file, \"w\") as f:\n",
    "        data = []\n",
    "        for triplet in triplet_sampler.iter_triplets_rand(10):\n",
    "            #print(json.dumps(triplet, indent=2))\n",
    "            data.append(triplet)\n",
    "        \n",
    "        # Save the triplets to disk\n",
    "        print(f\"Saving batch {batch_id} to disk\")\n",
    "        #f.write(json.dumps(data))\n",
    "        torch.save(data, file)\n",
    "        pbar.update(1)"
   ],
   "id": "214eeacd0dc41e7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "triplet_sampler = TripletSampler()\n",
    "path = \"./data/train_set\"\n",
    "import json\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Delete folder contents\n",
    "for file in os.listdir(path):\n",
    "    os.remove(os.path.join(path, file))\n",
    "\n",
    "num_ds = 300\n",
    "with tqdm(total=num_ds) as pbar:\n",
    "    for batch_id in range(num_ds):\n",
    "        file = os.path.join(path, f\"author_{batch_id}.pt\")\n",
    "        #with open(file, \"w\") as f:\n",
    "        data = []\n",
    "        author_id, normal_data, outliers = triplet_sampler.sample_author()\n",
    "        if author_id is None:\n",
    "            print(\"No more author ids to sample.\")\n",
    "            break\n",
    "        #print(json.dumps(triplet, indent=2))\n",
    "        normal_graphs, node_id_maps = sample_graphs(normal_data)\n",
    "        if normal_graphs is None:\n",
    "            continue\n",
    "            \n",
    "        outlier_graphs, outlier_node_id_maps = sample_graphs(outliers)\n",
    "        if outlier_graphs is None:\n",
    "            continue\n",
    "            \n",
    "        data.append({\n",
    "            \"author_id\": author_id,\n",
    "            \"normal_data\": {\n",
    "                \"graphs\": normal_graphs,\n",
    "                \"node_id_maps\": node_id_maps,\n",
    "                \"start_node_ids\": normal_data\n",
    "            },\n",
    "            \"outliers\": {\n",
    "                \"graphs\": outlier_graphs,\n",
    "                \"node_id_maps\": outlier_node_id_maps,\n",
    "                \"start_node_ids\": outliers\n",
    "            }\n",
    "        })\n",
    "    \n",
    "        # Save the triplets to disk\n",
    "        print(f\"Saving batch {batch_id} to disk\")\n",
    "        #f.write(json.dumps(data))\n",
    "        torch.save(data, file)\n",
    "        pbar.update(1)"
   ],
   "id": "79e7207a991d75ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fbf34bd4596940b5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
