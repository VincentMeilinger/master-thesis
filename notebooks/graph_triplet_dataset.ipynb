{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:32.291531Z",
     "start_time": "2024-08-25T19:15:32.285061Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j import GraphDatabase\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.datasets.who_is_who import WhoIsWhoDataset\n",
    "from src.model.GAT.gat_encoder import GATv2Encoder\n",
    "from src.model.GAT.gat_decoder import GATv2Decoder\n",
    "from src.shared.graph_schema import NodeType, EdgeType, node_one_hot, edge_one_hot\n",
    "from src.shared import config\n",
    "\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:32.295818Z",
     "start_time": "2024-08-25T19:15:32.293010Z"
    }
   },
   "cell_type": "code",
   "source": "driver = GraphDatabase.driver(config.DB_URI, auth=(config.DB_USER, config.DB_PASSWORD))",
   "id": "3a6f87824453abb4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:32.303921Z",
     "start_time": "2024-08-25T19:15:32.297318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_n_hop_neighbourhood(\n",
    "        start_node_type: NodeType, \n",
    "        start_node_id: str,\n",
    "        node_attr: str, \n",
    "        node_types: list = None, \n",
    "        edge_types: list = None,\n",
    "        max_level: int = 6\n",
    "):\n",
    "    with driver.session() as session:\n",
    "        node_filter = '|'.join(\n",
    "            [nt.value for nt in NodeType] if node_types is None else \n",
    "            [nt.value for nt in node_types]\n",
    "        )\n",
    "        edge_filter = '|'.join(\n",
    "            [f\"<{et.value}\" for et in EdgeType] if edge_types is None else \n",
    "            [f\"<{et.value}\" for et in edge_types]\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "                MATCH (start:{start_node_type.value} {{id: '{start_node_id}'}})\n",
    "                CALL apoc.path.subgraphAll(start, {{\n",
    "                  maxLevel: {max_level},\n",
    "                  relationshipFilter: '{edge_filter}',\n",
    "                  labelFilter: '+{node_filter}'\n",
    "                }}) YIELD nodes, relationships\n",
    "                RETURN nodes, relationships\n",
    "            \"\"\"\n",
    "        result = session.run(query)\n",
    "        data = result.single()\n",
    "        if not data:\n",
    "            return None, None, None\n",
    "        \n",
    "        nodes = data[\"nodes\"]\n",
    "        relationships = data[\"relationships\"]\n",
    "        \n",
    "        # Process nodes\n",
    "        x_dict = {}\n",
    "        x_ids = {}\n",
    "        for node in nodes:\n",
    "            node_id = node.get(\"id\")\n",
    "            node_feature = node.get(node_attr, None)\n",
    "            if node_feature is None:\n",
    "                print(f\"Node {node_id} has no attribute {node_attr}\")\n",
    "                continue\n",
    "            node_label = list(node.labels)[0]\n",
    "            if node_label not in x_dict:\n",
    "                x_dict[node_label] = []\n",
    "                x_ids[node_label] = []\n",
    "            \n",
    "            x_dict[node_label].append(torch.tensor(node_feature))\n",
    "            x_ids[node_label].append(node_id)\n",
    "        \n",
    "        for node_label, node_features in x_dict.items():\n",
    "            x_dict[node_label] = torch.vstack(node_features)\n",
    "            \n",
    "        # Process relationships\n",
    "        edge_dict = {}\n",
    "        \n",
    "        for rel in relationships:\n",
    "            if rel.type not in edge_dict:\n",
    "                edge_dict[rel.type] = [[], []]\n",
    "            source_id = rel.start_node.get(\"id\")\n",
    "            target_id = rel.end_node.get(\"id\")\n",
    "            \n",
    "            edge_dict[rel.type][0].append(source_id)\n",
    "            edge_dict[rel.type][1].append(target_id)\n",
    "        \n",
    "    return x_dict, x_ids, edge_dict"
   ],
   "id": "76af4d06aa310b6b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:32.310562Z",
     "start_time": "2024-08-25T19:15:32.304904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_topology(new_idx_to_old, topology):\n",
    "    # Reverse index mapping based on new idx -> old idx\n",
    "    old_idx_to_new = dict((v, k) for k, v in new_idx_to_old.items())\n",
    "    return {rel_type: [[old_idx_to_new[node_id] for node_id in nodes] for nodes in topology] for rel_type, topology in topology.items()}\n",
    "\n",
    "def create_edge_index(topology):\n",
    "    edge_index = []\n",
    "    edge_features = []\n",
    "    edge_labels = []\n",
    "    for rel_type, nodes in topology.items():\n",
    "        src_nodes, dst_nodes = nodes\n",
    "        edges = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)\n",
    "        edge_index.append(edges)\n",
    "        # Extend edge features by one-hot encoding of the relationship type\n",
    "        edge_features.extend([edge_one_hot[rel_type] for _ in range(len(src_nodes))])\n",
    "        # Extend edge labels by the relationship type for each edge feature vector\n",
    "        edge_labels.extend([rel_type for _ in range(len(src_nodes))])\n",
    "    return torch.cat(edge_index, dim=1), torch.vstack(edge_features), edge_labels\n",
    "\n",
    "def remap_node_indices(x_ids: dict, edge_dict: dict):\n",
    "    unique_node_ids = list(set([node_id for id_list in x_ids.values() for node_id in id_list]))\n",
    "    old_id_to_new = {node_id: i for i, node_id in enumerate(unique_node_ids)}\n",
    "    new_edge_index_dict = {}\n",
    "    \n",
    "    for rel_type, node_ids in edge_dict.items():\n",
    "        src_nodes, dest_nodes = node_ids\n",
    "        edge_index = []\n",
    "        for src, dest in zip(src_nodes, dest_nodes):\n",
    "            edge_index.append(torch.tensor([old_id_to_new[src], old_id_to_new[dest]], dtype=torch.long))\n",
    "        new_edge_index_dict[rel_type] = torch.vstack(edge_index).T\n",
    "        \n",
    "    return new_edge_index_dict\n",
    "              \n",
    "def create_edge_feature_dict(edge_dict):\n",
    "    edge_feature_dict = {}\n",
    "    for rel_type, node_ids in edge_dict.items():\n",
    "        num_edges = len(node_ids[0])\n",
    "        edge_features = [edge_one_hot[rel_type] for _ in range(num_edges)]\n",
    "        edge_feature_dict[rel_type] = torch.vstack(edge_features)\n",
    "        \n",
    "    return edge_feature_dict"
   ],
   "id": "6089817ba6f16189",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:32.316234Z",
     "start_time": "2024-08-25T19:15:32.312043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "included_nodes = [\n",
    "    NodeType.PUBLICATION, \n",
    "    NodeType.VENUE, \n",
    "    NodeType.ORGANIZATION,\n",
    "    NodeType.AUTHOR,\n",
    "    NodeType.CO_AUTHOR\n",
    "]\n",
    "included_edges = [\n",
    "    EdgeType.PUB_VENUE,\n",
    "    EdgeType.VENUE_PUB,\n",
    "    EdgeType.PUB_ORG,\n",
    "    EdgeType.ORG_PUB, \n",
    "    #EdgeType.SIM_VENUE,\n",
    "    #EdgeType.SIM_ORG,\n",
    "    EdgeType.PUB_AUTHOR,\n",
    "    EdgeType.AUTHOR_PUB,\n",
    "    EdgeType.AUTHOR_ORG,\n",
    "    EdgeType.ORG_AUTHOR,\n",
    "    EdgeType.PUB_ORG,\n",
    "    EdgeType.ORG_PUB,\n",
    "]\n",
    "\n",
    "def sample_triplet(anchor, pos, neg):\n",
    "    triplet = {}\n",
    "    for label, node_id in zip([\"anchor\", \"pos\", \"neg\"], [anchor, pos, neg]):\n",
    "        x_dict, x_ids, edge_dict = fetch_n_hop_neighbourhood(\n",
    "            start_node_type=NodeType.PUBLICATION, \n",
    "            start_node_id=node_id, \n",
    "            node_attr=\"vec\",\n",
    "            node_types=included_nodes,\n",
    "            edge_types=included_edges,\n",
    "            max_level=2\n",
    "        )\n",
    "        if x_dict is None or len(x_dict.keys()) == 0:\n",
    "            return None\n",
    "        if edge_dict is None or len(edge_dict.keys()) == 0:\n",
    "            return None\n",
    "            \n",
    "        edge_index = remap_node_indices(x_ids, edge_dict)\n",
    "        edge_features = create_edge_feature_dict(edge_dict)\n",
    "        \n",
    "        graph = {\n",
    "            \"x_dict\": x_dict,\n",
    "            \"x_ids\": x_ids,\n",
    "            \"edge_index_dict\": edge_index,\n",
    "            \"edge_feature_dict\": edge_features,\n",
    "        }\n",
    "\n",
    "        triplet[label] = graph\n",
    "    \n",
    "    return triplet"
   ],
   "id": "b2b1f2cab33a61f0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:32.322198Z",
     "start_time": "2024-08-25T19:15:32.317032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TripleSampler:\n",
    "    def __init__(self):\n",
    "        self.data = WhoIsWhoDataset.parse_train()\n",
    "        \n",
    "    def iter_triplet_ids_rand(self, num_triplets):\n",
    "        # yield random triplets from the WhoIsWho dataset\n",
    "        current_num_triplets = 0\n",
    "        while current_num_triplets < num_triplets:\n",
    "            # Get random author\n",
    "            author_id = random.choice(list(self.data.keys()))\n",
    "            \n",
    "            # Check if author has enough data\n",
    "            normal_data = self.data[author_id][\"normal_data\"]\n",
    "            outliers = self.data[author_id][\"outliers\"]\n",
    "            if len(normal_data) < 2 or len(outliers) == 0:\n",
    "                self.data.pop(author_id)\n",
    "                continue\n",
    "            # Get random anchor, positive and negative samples, remove anchor\n",
    "            anchor_id = random.choice(normal_data)\n",
    "            self.data[author_id][\"normal_data\"].remove(anchor_id)\n",
    "            \n",
    "            pos_sample = random.choice(normal_data)\n",
    "            neg_sample = random.choice(outliers)\n",
    "            yield anchor_id, pos_sample, neg_sample\n",
    "            current_num_triplets += 1\n",
    "            \n",
    "    def sample_triplet_ids(self, attempt = 0):\n",
    "        if attempt > 30:\n",
    "            print(\"Unable to sample more triples\")\n",
    "            return None\n",
    "        # Get random author\n",
    "        author_id = random.choice(list(self.data.keys()))\n",
    "        \n",
    "        # Check if author has enough data\n",
    "        normal_data = self.data[author_id][\"normal_data\"]\n",
    "        outliers = self.data[author_id][\"outliers\"]\n",
    "        if len(normal_data) < 2 or len(outliers) == 0:\n",
    "            self.data.pop(author_id)\n",
    "            return self.sample_triplet_ids(attempt + 1)\n",
    "        # Get random anchor, positive and negative samples, remove anchor\n",
    "        anchor_id = random.choice(normal_data)\n",
    "        self.data[author_id][\"normal_data\"].remove(anchor_id)\n",
    "        \n",
    "        pos_sample = random.choice(normal_data)\n",
    "        neg_sample = random.choice(outliers)\n",
    "        return anchor_id, pos_sample, neg_sample\n",
    "\n",
    "    def iter_triplets_rand(self, num_triplets):\n",
    "        # Fetch triplets by id from the neo4j database and yield them\n",
    "        count = 0\n",
    "        count_skipped = 0\n",
    "        while count < num_triplets:                \n",
    "            anchor_id, pos_id, neg_id = self.sample_triplet_ids()\n",
    "            triplet = sample_triplet(anchor_id, pos_id, neg_id)\n",
    "            if triplet is None or type(triplet) is not dict:\n",
    "                count_skipped += 1\n",
    "                continue\n",
    "        \n",
    "            yield triplet\n",
    "            count += 1\n",
    "        #print(f\"Requested {num_triplets}, Fetched {count}, Skipped {count_skipped}\")"
   ],
   "id": "b101664d258f7f0e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T19:15:38.548897Z",
     "start_time": "2024-08-25T19:15:32.323236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "triplet_sampler = TripleSampler()\n",
    "path = \"./data/triplet_dataset\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Delete folder contents\n",
    "for file in os.listdir(path):\n",
    "    os.remove(os.path.join(path, file))\n",
    "\n",
    "for batch_id in range(10000):\n",
    "    file = os.path.join(path, f\"triplet_batch_{batch_id}.pt\")\n",
    "    #with open(file, \"w\") as f:\n",
    "    data = []\n",
    "    for triplet in triplet_sampler.iter_triplets_rand(10):\n",
    "        #print(json.dumps(triplet, indent=2))\n",
    "        data.append(triplet)\n",
    "    \n",
    "    # Save the triplets to disk\n",
    "    print(f\"Saving batch {batch_id} to disk\")\n",
    "    #f.write(json.dumps(data))\n",
    "    torch.save(data, file)"
   ],
   "id": "70de02ffe6f0d407",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving batch 0 to disk\n",
      "Saving batch 1 to disk\n",
      "Saving batch 2 to disk\n",
      "Saving batch 3 to disk\n",
      "Saving batch 4 to disk\n",
      "Saving batch 5 to disk\n",
      "Saving batch 6 to disk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#with open(file, \"w\") as f:\u001B[39;00m\n\u001B[1;32m     14\u001B[0m data \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m triplet \u001B[38;5;129;01min\u001B[39;00m triplet_sampler\u001B[38;5;241m.\u001B[39miter_triplets_rand(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m#print(json.dumps(triplet, indent=2))\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     data\u001B[38;5;241m.\u001B[39mappend(triplet)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Save the triplets to disk\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[20], line 54\u001B[0m, in \u001B[0;36mTripleSampler.iter_triplets_rand\u001B[0;34m(self, num_triplets)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m count \u001B[38;5;241m<\u001B[39m num_triplets:                \n\u001B[1;32m     53\u001B[0m     anchor_id, pos_id, neg_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample_triplet_ids()\n\u001B[0;32m---> 54\u001B[0m     triplet \u001B[38;5;241m=\u001B[39m \u001B[43msample_triplet\u001B[49m\u001B[43m(\u001B[49m\u001B[43manchor_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m triplet \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(triplet) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m     56\u001B[0m         count_skipped \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[19], line 26\u001B[0m, in \u001B[0;36msample_triplet\u001B[0;34m(anchor, pos, neg)\u001B[0m\n\u001B[1;32m     24\u001B[0m triplet \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label, node_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manchor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mneg\u001B[39m\u001B[38;5;124m\"\u001B[39m], [anchor, pos, neg]):\n\u001B[0;32m---> 26\u001B[0m     x_dict, x_ids, edge_dict \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_n_hop_neighbourhood\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstart_node_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNodeType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPUBLICATION\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstart_node_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnode_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnode_attr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvec\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnode_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mincluded_nodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43medge_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mincluded_edges\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x_dict\u001B[38;5;241m.\u001B[39mkeys()) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     35\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[17], line 29\u001B[0m, in \u001B[0;36mfetch_n_hop_neighbourhood\u001B[0;34m(start_node_type, start_node_id, node_attr, node_types, edge_types, max_level)\u001B[0m\n\u001B[1;32m     19\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124m        MATCH (start:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_node_type\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[38;5;124mid: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_node_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124m        CALL apoc.path.subgraphAll(start, \u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124m        RETURN nodes, relationships\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     28\u001B[0m result \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mrun(query)\n\u001B[0;32m---> 29\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msingle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/work/result.py:485\u001B[0m, in \u001B[0;36mResult.single\u001B[0;34m(self, strict)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;129m@NonConcurrentMethodChecker\u001B[39m\u001B[38;5;241m.\u001B[39mnon_concurrent_method\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msingle\u001B[39m(\u001B[38;5;28mself\u001B[39m, strict: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mOptional[Record]:\n\u001B[1;32m    451\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Obtain the next and only remaining record or None.\u001B[39;00m\n\u001B[1;32m    452\u001B[0m \n\u001B[1;32m    453\u001B[0m \u001B[38;5;124;03m    Calling this method always exhausts the result.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;124;03m        * Can raise :exc:`.ResultConsumedError`.\u001B[39;00m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 485\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    486\u001B[0m     buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_record_buffer\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_record_buffer \u001B[38;5;241m=\u001B[39m deque()\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/work/result.py:318\u001B[0m, in \u001B[0;36mResult._buffer\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    317\u001B[0m record_buffer \u001B[38;5;241m=\u001B[39m deque()\n\u001B[0;32m--> 318\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m record \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m    319\u001B[0m     record_buffer\u001B[38;5;241m.\u001B[39mappend(record)\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(record_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m n:\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/work/result.py:270\u001B[0m, in \u001B[0;36mResult.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_record_buffer\u001B[38;5;241m.\u001B[39mpopleft()\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_streaming:\n\u001B[0;32m--> 270\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_discarding:\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_discard()\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:178\u001B[0m, in \u001B[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 178\u001B[0m         \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    180\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39miscoroutinefunction(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__on_error)\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/io/_bolt.py:846\u001B[0m, in \u001B[0;36mBolt.fetch_message\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    843\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;66;03m# Receive exactly one message\u001B[39;00m\n\u001B[0;32m--> 846\u001B[0m tag, fields \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhydration_hooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponses\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhydration_hooks\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    849\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_message(tag, fields)\n\u001B[1;32m    850\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39midle_since \u001B[38;5;241m=\u001B[39m monotonic()\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:72\u001B[0m, in \u001B[0;36mInbox.pop\u001B[0;34m(self, hydration_hooks)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpop\u001B[39m(\u001B[38;5;28mself\u001B[39m, hydration_hooks):\n\u001B[0;32m---> 72\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer_one_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m         size, tag \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_unpacker\u001B[38;5;241m.\u001B[39munpack_structure_header()\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:51\u001B[0m, in \u001B[0;36mInbox._buffer_one_chunk\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m chunk_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;66;03m# Determine the chunk size and skip noop\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m         \u001B[43mreceive_into_buffer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_socket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m         chunk_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer\u001B[38;5;241m.\u001B[39mpop_u16()\n\u001B[1;32m     53\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m chunk_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:326\u001B[0m, in \u001B[0;36mreceive_into_buffer\u001B[0;34m(sock, buffer, n_bytes)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(buffer\u001B[38;5;241m.\u001B[39mdata) \u001B[38;5;28;01mas\u001B[39;00m view:\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m buffer\u001B[38;5;241m.\u001B[39mused \u001B[38;5;241m<\u001B[39m end:\n\u001B[0;32m--> 326\u001B[0m         n \u001B[38;5;241m=\u001B[39m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mview\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mused\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mused\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_async_compat/network/_bolt_socket.py:493\u001B[0m, in \u001B[0;36mBoltSocket.recv_into\u001B[0;34m(self, buffer, nbytes)\u001B[0m\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrecv_into\u001B[39m(\u001B[38;5;28mself\u001B[39m, buffer, nbytes):\n\u001B[0;32m--> 493\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_io\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_socket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/master/lib/python3.9/site-packages/neo4j/_async_compat/network/_bolt_socket.py:468\u001B[0m, in \u001B[0;36mBoltSocket._wait_for_io\u001B[0;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wait_for_io\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deadline \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 468\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    469\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_socket\u001B[38;5;241m.\u001B[39mgettimeout()\n\u001B[1;32m    470\u001B[0m     deadline_timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deadline\u001B[38;5;241m.\u001B[39mto_timeout()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "39b558c7a655980f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
