{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:21:03.116110Z",
     "start_time": "2024-10-13T17:21:00.346469Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "import sentence_transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.pipeline.transformer_dim_reduction import *"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincie/.anaconda3/envs/master/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:21:03.129041Z",
     "start_time": "2024-10-13T17:21:03.123400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _compare_performance(emb1, emb2):\n",
    "    # Compare the similarity computation performance of the two models\n",
    "    sim1 = cosine_similarity(emb1)\n",
    "    sim2 = cosine_similarity(emb2)\n",
    "    difference_matrix = np.abs(sim2 - sim1)\n",
    "    return np.mean(difference_matrix), np.std(difference_matrix)\n",
    "\n",
    "\n",
    "def edr_eval(train, full_emb, new_dimension: int, model_name: str):\n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings ...\")\n",
    "    model = SentenceTransformer(\n",
    "        model_name,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "    # PCA on train embeddings\n",
    "    print(\"Performing PCA on train embeddings ...\")\n",
    "    pca = PCA(n_components=new_dimension)\n",
    "    pca.fit(full_emb)\n",
    "    pca_comp = np.asarray(pca.components_)\n",
    "\n",
    "    # Add a dense layer to the model\n",
    "    print(\"Adding dense layer to the model ...\")\n",
    "    dense = models.Dense(\n",
    "        in_features=model.get_sentence_embedding_dimension(),\n",
    "        out_features=new_dimension,\n",
    "        bias=False,\n",
    "        activation_function=torch.nn.Identity(),\n",
    "    )\n",
    "    dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))\n",
    "    model.add_module(\"dense\", dense)\n",
    "\n",
    "    # Evaluate the model with the reduce embedding size\n",
    "    print(f\"Model with {new_dimension} dimensions:\")\n",
    "    red_emb = model.encode(train, convert_to_numpy=True)\n",
    "    mean_diff, std_diff = _compare_performance(full_emb, red_emb)\n",
    "    print(f\"Mean difference: {mean_diff}, Std difference: {std_diff}\")\n",
    "\n",
    "    # Store the model on disc\n",
    "    model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n",
    "    os.makedirs(\"data/models\", exist_ok=True)\n",
    "    model.save(f\"data/models/{model_name}-{new_dimension}dim\")\n",
    "\n",
    "\n",
    "def reduce_transformer_dim(base_model_name: str):\n",
    "    print(\"Adding a dense layer to the transformer model to reduce the embedding dimension ...\")\n",
    "\n",
    "    # Parse datasets\n",
    "    abstracts = []\n",
    "    data = WhoIsWhoDataset.parse_data()\n",
    "\n",
    "    for paper_id, paper_info in data.items():\n",
    "        abstracts.append(paper_info['abstract'])\n",
    "\n",
    "    # Get train, test, valid splits\n",
    "    print(\"Splitting data into train, test, valid ...\")\n",
    "    random.shuffle(abstracts)\n",
    "    train_size = int(0.8 * len(abstracts))\n",
    "    test_size = int(0.1 * len(abstracts))\n",
    "    train, test, valid = abstracts[:train_size], abstracts[train_size:train_size + test_size], abstracts[\n",
    "                                                                                               train_size + test_size:]\n",
    "    max_samples = int(10000)\n",
    "    train = train[0:max_samples]\n",
    "\n",
    "    # Embed train data using full model for comparison\n",
    "    print('Embedding data using full model ...')\n",
    "    full_model = sentence_transformers.SentenceTransformer(\n",
    "        base_model_name,\n",
    "        device='cuda'\n",
    "    )\n",
    "    start = time()\n",
    "    full_emb = full_model.encode(train, convert_to_numpy=True)\n",
    "    print(f\"Full model embedding time: {time() - start}s\")\n",
    "\n",
    "    # Reduce the dimensionality of the embeddings, evaluate the performance compared to the full model\n",
    "    edr_eval(\n",
    "        train,\n",
    "        full_emb,\n",
    "        new_dimension=32,\n",
    "        model_name=base_model_name\n",
    "    )\n"
   ],
   "id": "ebe780616c2271a6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:21:41.341853Z",
     "start_time": "2024-10-13T17:21:03.165639Z"
    }
   },
   "cell_type": "code",
   "source": "reduce_transformer_dim('sentence-transformers/all-MiniLM-L6-v2')",
   "id": "e01332559164376c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding a dense layer to the transformer model to reduce the embedding dimension ...\n",
      "Splitting data into train, test, valid ...\n",
      "Embedding data using full model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincie/.anaconda3/envs/master/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model embedding time: 11.061969995498657s\n",
      "Generating embeddings ...\n",
      "Performing PCA on train embeddings ...\n",
      "Adding dense layer to the model ...\n",
      "Model with 32 dimensions:\n",
      "Mean difference: 0.06308086216449738, Std difference: 0.06814222037792206\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
