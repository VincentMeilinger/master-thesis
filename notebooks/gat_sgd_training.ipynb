{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j import GraphDatabase\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from src.datasets.who_is_who import WhoIsWhoDataset\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.model.GAT.gat_encoder import GATv2Encoder\n",
    "from src.model.loss.triplet_loss import TripletLoss\n",
    "from src.model.GAT.gat_decoder import GATv2Decoder\n",
    "from src.shared.graph_schema import NodeType, EdgeType, node_one_hot, edge_one_hot\n",
    "from src.shared import config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ],
   "id": "f4212cea93939031",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_topology(new_idx_to_old, topology):\n",
    "    # Reverse index mapping based on new idx -> old idx\n",
    "    old_idx_to_new = dict((v, k) for k, v in new_idx_to_old.items())\n",
    "    return {rel_type: [[old_idx_to_new[node_id] for node_id in nodes] for nodes in topology] for rel_type, topology in topology.items()}\n",
    "\n",
    "def create_edge_index(topology):\n",
    "    edge_index = []\n",
    "    edge_features = []\n",
    "    for rel_type, nodes in topology.items():\n",
    "        src_nodes, dst_nodes = nodes\n",
    "        edges = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)\n",
    "        edge_index.append(edges)\n",
    "        edge_feature_vec = edge_one_hot[rel_type]\n",
    "        edge_features.extend([edge_feature_vec for _ in range(len(src_nodes))])\n",
    "    return torch.cat(edge_index, dim=1), torch.vstack(edge_features)\n",
    "\n",
    "def project_node_embeddings(node_df):\n",
    "    def stack_one_hot(row):\n",
    "        one_hot_enc = node_one_hot[row[\"nodeLabels\"][0]]\n",
    "        return torch.hstack((one_hot_enc, torch.tensor(row[\"vec\"])))\n",
    "    return node_df.apply(stack_one_hot, axis=1)\n"
   ],
   "id": "7af7c76c9910e14a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "driver = GraphDatabase.driver(config.DB_URI, auth=(config.DB_USER, config.DB_PASSWORD))\n",
    "\n",
    "def fetch_n_hop_neighbourhood(start_node_type: NodeType, start_node_id: str, node_attr: str, node_types: list = None, edge_types: list = None, max_level: int = 1):\n",
    "    with driver.session() as session:\n",
    "        node_filter = '|'.join(\n",
    "            [nt.value for nt in NodeType] if node_types is None else \n",
    "            [nt.value for nt in node_types]\n",
    "        )\n",
    "        edge_filter = '|'.join(\n",
    "            [f\"<{et.value}\" for et in EdgeType] if edge_types is None else\n",
    "            [f\"<{et.value}\" for et in edge_types]\n",
    "        )\n",
    "\n",
    "        query = f\"\"\"\n",
    "                MATCH (start:{start_node_type.value} {{id: '{start_node_id}'}})\n",
    "                CALL apoc.path.subgraphAll(start, {{\n",
    "                  maxLevel: {max_level},\n",
    "                  relationshipFilter: '{edge_filter}',\n",
    "                  labelFilter: '+{node_filter}'\n",
    "                }}) YIELD nodes, relationships\n",
    "                RETURN nodes, relationships\n",
    "            \"\"\"\n",
    "        result = session.run(query)\n",
    "        data = result.single()\n",
    "        try:\n",
    "            nodes = data[\"nodes\"]\n",
    "            relationships = data[\"relationships\"]\n",
    "    \n",
    "            # Process nodes\n",
    "            node_data = []\n",
    "            for node in nodes:\n",
    "                node_id = node.get(\"id\")\n",
    "                attr = node.get(node_attr, None)\n",
    "                node_data.append({\"nodeId\": node_id, node_attr: attr, \"nodeLabels\": list(node.labels)})\n",
    "    \n",
    "            node_df = pd.DataFrame(node_data)\n",
    "    \n",
    "            # Process relationships\n",
    "            edge_dict = {}\n",
    "            for rel in relationships:\n",
    "                if rel.type not in edge_dict:\n",
    "                    edge_dict[rel.type] = [[], []]\n",
    "                source_id = rel.start_node.get(\"id\")\n",
    "                target_id = rel.end_node.get(\"id\")\n",
    "    \n",
    "                edge_dict[rel.type][0].append(source_id)\n",
    "                edge_dict[rel.type][1].append(target_id)\n",
    "        except Exception as e:\n",
    "            #print(f\"Error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    return node_df, edge_dict"
   ],
   "id": "70f6cfc123b440d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "included_nodes = [\n",
    "    NodeType.PUBLICATION,\n",
    "    NodeType.VENUE,\n",
    "    NodeType.ORGANIZATION,\n",
    "    NodeType.AUTHOR,\n",
    "    NodeType.CO_AUTHOR\n",
    "]\n",
    "included_edges = [\n",
    "    EdgeType.PUB_VENUE,\n",
    "    EdgeType.PUB_ORG,\n",
    "    EdgeType.SIM_VENUE,\n",
    "    EdgeType.SIM_ORG,\n",
    "    EdgeType.ORG_PUB,\n",
    "    EdgeType.VENUE_PUB,\n",
    "    EdgeType.PUB_AUTHOR,\n",
    "    EdgeType.AUTHOR_PUB,\n",
    "    EdgeType.AUTHOR_CO_AUTHOR,\n",
    "    EdgeType.CO_AUTHOR_AUTHOR,\n",
    "    EdgeType.PUB_CO_AUTHOR,\n",
    "    EdgeType.CO_AUTHOR_PUB,\n",
    "    EdgeType.AUTHOR_ORG,\n",
    "    EdgeType.ORG_AUTHOR,\n",
    "    EdgeType.CO_AUTHOR_ORG,\n",
    "    EdgeType.ORG_CO_AUTHOR\n",
    "]\n",
    "\n",
    "def sample_subgraph(node_list):\n",
    "    dataset = []\n",
    "    for node_id in node_list:\n",
    "        node_df, topology = fetch_n_hop_neighbourhood(\n",
    "            start_node_type=NodeType.PUBLICATION, \n",
    "            start_node_id=node_id, \n",
    "            node_attr=\"vec\",\n",
    "            node_types=included_nodes,\n",
    "            edge_types=included_edges,\n",
    "            max_level=3\n",
    "        )\n",
    "        if node_df is None or len(node_df) == 0:\n",
    "            continue\n",
    "        node_df[\"vec_projected\"] = project_node_embeddings(node_df)\n",
    "        normalized_node_ids = {new_idx: old_idx for new_idx, old_idx in enumerate(node_df[\"nodeId\"])}\n",
    "        normalized_topology = normalize_topology(normalized_node_ids, topology)\n",
    "        if not normalized_topology or len(normalized_topology) == 0:\n",
    "            continue\n",
    "            \n",
    "        edge_index, edge_features = create_edge_index(normalized_topology)\n",
    "        node_features = torch.vstack(node_df[\"vec_projected\"].tolist())\n",
    "        \n",
    "        dataset.append(Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_features\n",
    "        ))\n",
    "    return dataset\n",
    "\n",
    "def sample_triplet(triplet_ids):\n",
    "    triplet = []\n",
    "    for node_id in triplet_ids:\n",
    "        node_df, topology = fetch_n_hop_neighbourhood(\n",
    "            start_node_type=NodeType.PUBLICATION, \n",
    "            start_node_id=node_id, \n",
    "            node_attr=\"vec\",\n",
    "            node_types=included_nodes,\n",
    "            edge_types=included_edges,\n",
    "            max_level=2\n",
    "        )\n",
    "        if node_df is None or len(node_df) == 0:\n",
    "            continue\n",
    "        node_df[\"vec_projected\"] = project_node_embeddings(node_df)\n",
    "        normalized_node_ids = {new_idx: old_idx for new_idx, old_idx in enumerate(node_df[\"nodeId\"])}\n",
    "        normalized_topology = normalize_topology(normalized_node_ids, topology)\n",
    "        if not normalized_topology or len(normalized_topology) == 0:\n",
    "            continue\n",
    "            \n",
    "        edge_index, edge_features = create_edge_index(normalized_topology)\n",
    "        node_features = torch.vstack(node_df[\"vec_projected\"].tolist())\n",
    "        \n",
    "        triplet.append(Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_features\n",
    "        ))\n",
    "        print(f\"Sampled triplet {node_id}\")\n",
    "        print(f\"Num nodes: {len(node_df)}, node features: {node_features.shape}\")\n",
    "        print(f\"Num edges: {len(edge_index)}, edge features: {edge_features.shape}\")\n",
    "    if len(triplet) != 3:\n",
    "        return None\n",
    "    return triplet"
   ],
   "id": "16465ddeb3ddcffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_wrapper = DatabaseWrapper()\n",
    "start_nodes = []\n",
    "\n",
    "def iter_triplet_ids_rand(max_triplets = 1000):\n",
    "    # yield random triplets from the WhoIsWho dataset\n",
    "    data = WhoIsWhoDataset.parse_train()\n",
    "    for i in range(max_triplets):\n",
    "        author_id = random.choice(list(data.keys()))\n",
    "        anchor_id = random.choice(data[author_id][\"normal_data\"])\n",
    "        data[author_id][\"normal_data\"].remove(anchor_id)\n",
    "        pos_data = data[author_id][\"normal_data\"]\n",
    "        neg_data = data[author_id][\"outliers\"]\n",
    "        if len(pos_data) < 2 or len(neg_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        pos_sample = random.sample(pos_data, 1)\n",
    "        neg_sample = random.sample(neg_data, 1)\n",
    "        yield anchor_id, pos_sample[0], neg_sample[0]\n",
    "\n",
    "def iter_triplets_rand(max_triplets = 1000):\n",
    "    # Fetch triplets by id from the neo4j database and yield them\n",
    "    count = 0\n",
    "    for anchor_id, pos_id, neg_id in iter_triplet_ids_rand(max_triplets * 10):\n",
    "        print(f\"Fetching triplet {anchor_id}, {pos_id}, {neg_id}\")\n",
    "        if count > max_triplets:\n",
    "            break\n",
    "        triplet = sample_triplet([anchor_id, pos_id, neg_id])\n",
    "        if not triplet:\n",
    "            print(f\"Skipping triplet {anchor_id}, {pos_id}, {neg_id}\")\n",
    "            continue\n",
    "        yield triplet\n",
    "        count += 1\n",
    "        \n",
    "        "
   ],
   "id": "dc0cbea84e93fb80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "node_feature_dim = NodeType.PUBLICATION.one_hot().shape[0] + 32\n",
    "edge_feature_dim = EdgeType.SIM_TITLE.one_hot().shape[0]\n",
    "gat_embedding_dim = 32\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(device)\n",
    "\n",
    "encoder = GATv2Encoder(\n",
    "    in_channels=node_feature_dim,\n",
    "    hidden_channels=32,\n",
    "    out_channels=gat_embedding_dim,\n",
    "    edge_dim=edge_feature_dim,\n",
    "    add_self_loops=False\n",
    ")\n",
    "encoder.to(device)"
   ],
   "id": "65cda043db9a88fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_gat(encoder, dataloader, epochs=1000, lr=0.01):\n",
    "    # Define the optimizer for the gat model\n",
    "    optimizer = optim.SGD(list(encoder.parameters()), lr=lr)\n",
    "    \n",
    "    # Define a loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    triplet_loss = TripletLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            anchor, pos, neg = batch\n",
    "            print(anchor)\n",
    "            print(pos)\n",
    "            print(neg)\n",
    "            print(anchor.x)\n",
    "            print(anchor.edge_index)\n",
    "            print(anchor.edge_attr)\n",
    "            anchor.to(device)\n",
    "            pos.to(device)\n",
    "            neg.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the encoder\n",
    "            anchor_emb = encoder(anchor.x, anchor.edge_index, anchor.edge_attr)\n",
    "            pos_emb = encoder(pos.x, pos.edge_index, pos.edge_attr)\n",
    "            neg_emb = encoder(neg.x, neg.edge_index, neg.edge_attr)\n",
    "            \n",
    "            # Compute loss \n",
    "            #loss = criterion()\n",
    "            loss = triplet_loss.forward(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')"
   ],
   "id": "921a79247a6e9d92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "class TripletIterableDataset(IterableDataset):\n",
    "    def __init__(self, max_triplets=1000):\n",
    "        super(TripletIterableDataset, self).__init__()\n",
    "        self.max_triplets = max_triplets\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter_triplets_rand(self.max_triplets)\n",
    "    \n",
    "dataset = TripletIterableDataset(max_triplets=1000)\n",
    "dataloader = DataLoader(dataset, batch_size=3)\n",
    "train_gat(encoder, dataloader, epochs=1000, lr=0.01)"
   ],
   "id": "f34c647ef5332c1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "71b61a94d8513443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Idea: Predict links between papers purely based on graph structure depending on whether they were written by the same author",
   "id": "241a579e9f048ab7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
