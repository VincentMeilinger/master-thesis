{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.662124Z",
     "start_time": "2024-10-12T10:20:31.658050Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "from typing import Any\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.distributions.constraints import positive\n",
    "from torch_geometric.nn.models.dimenet import triplets\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j import GraphDatabase\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, global_mean_pool, HeteroConv\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.data import HeteroData\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.nn.modules.loss import TripletMarginLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Batch\n",
    "from torch.optim import Adam\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.datasets.who_is_who import WhoIsWhoDataset\n",
    "from src.model.GAT.gat_encoder import GATv2Encoder\n",
    "from src.model.GAT.gat_decoder import GATv2Decoder\n",
    "from src.shared.graph_schema import NodeType, EdgeType, node_one_hot, edge_one_hot, edge_pyg_key_vals\n",
    "from src.model.loss.triplet_loss import TripletLoss\n",
    "from src.shared import config\n",
    "from torch.utils.data import Dataset\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from itertools import combinations, product\n",
    "from src.shared.graph_sampling import GraphSampling\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "ee4fa502d7e51e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.706802Z",
     "start_time": "2024-10-12T10:20:31.704452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph sampling configurations\n",
    "node_spec = [\n",
    "    NodeType.PUBLICATION,\n",
    "]\n",
    "\n",
    "edge_spec = [\n",
    "    EdgeType.SIM_TITLE,\n",
    "    EdgeType.SIM_ABSTRACT,\n",
    "    EdgeType.SIM_VENUE,\n",
    "    EdgeType.SIM_AUTHOR,\n",
    "]\n",
    "\n",
    "node_properties = [\n",
    "    'id',\n",
    "    'title',\n",
    "    'abstract',\n",
    "    'venue',\n",
    "    'title_emb',\n",
    "    'abstract_emb',\n",
    "    'venue_emb',\n",
    "    'true_author',\n",
    "]\n",
    "\n",
    "gs = GraphSampling(\n",
    "    node_spec=node_spec,\n",
    "    edge_spec=edge_spec,\n",
    "    node_properties=node_properties,\n",
    "    database='homogeneous-graph',\n",
    ")\n",
    "\n",
    "# Model configurations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_node_feature = 'abstract_emb'  # Node feature to use for GAT encoder\n",
    "model_edge_type = EdgeType.SIM_AUTHOR  # Edge type to use for GAT encoder"
   ],
   "id": "91b04efb689d61f4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph Pair Data Handling",
   "id": "116d0a70795de1aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.753119Z",
     "start_time": "2024-10-12T10:20:31.749450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraphPairDataset(Dataset):\n",
    "    def __init__(self, pairs, gs):\n",
    "        self.pairs = pairs  # List of tuples: (paper_id1, paper_id2, label)\n",
    "        self.gs = gs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paper_id1, paper_id2, label = self.pairs[idx]\n",
    "        try:\n",
    "            #print(f\"Processing pair ({paper_id1}, {paper_id2})\")\n",
    "            # Get n-hop neighbourhood for each paper\n",
    "            graph1 = self.gs.n_hop_neighbourhood(NodeType.PUBLICATION, paper_id1, max_level=3)\n",
    "            graph2 = self.gs.n_hop_neighbourhood(NodeType.PUBLICATION, paper_id2, max_level=3)\n",
    "\n",
    "            # Convert to PyG Data objects\n",
    "            data1, node_map_1 = neo_to_pyg_homogeneous(graph1, model_node_feature)\n",
    "            data1.central_node_id = torch.tensor([node_map_1[paper_id1]])\n",
    "            \n",
    "            data2, node_map_2 = neo_to_pyg_homogeneous(graph2, model_node_feature)\n",
    "            data2.central_node_id = torch.tensor([node_map_2[paper_id2]])\n",
    "            \n",
    "            # Return data and label\n",
    "            return data1, data2, torch.tensor(label, dtype=torch.float)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair ({paper_id1}, {paper_id2}): {e}\")\n",
    "            return None\n",
    "        \n",
    "class PairData(Data):\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "        if key == 'central_node_id':\n",
    "            return 0  # Concat along batch dim\n",
    "        else:\n",
    "            return super().__cat_dim__(key, value, *args, **kwargs)\n",
    "        \n",
    "    def __inc__(self, key: str, value: Any, *args, **kwargs) -> Any:\n",
    "        if key == 'central_node_id':\n",
    "            return self.num_nodes\n",
    "        else:\n",
    "            return super().__inc__(key, value, *args, **kwargs)"
   ],
   "id": "6fed7b3a506508a0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.797852Z",
     "start_time": "2024-10-12T10:20:31.795391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graph_data_valid(data: Data):\n",
    "    try:\n",
    "        assert data is not None, \"Data object is None.\"\n",
    "        assert data.x is not None, \"Node features 'x' are missing.\"\n",
    "        assert data.edge_index is not None, \"Edge index 'edge_index' is missing.\"\n",
    "        assert data.num_nodes > 0, \"Number of nodes must be greater than 0.\"\n",
    "        assert data.num_edges > 0, \"Number of edges must be greater than 0.\"\n",
    "        assert data.x.size(0) == data.num_nodes, \"Mismatch between 'x' size and 'num_nodes'.\"\n",
    "        assert data.edge_index.size(0) == 2, \"'edge_index' should have shape [2, num_edges].\"\n",
    "        assert data.edge_index.size(1) == data.num_edges, \"Mismatch between 'edge_index' and 'num_edges'.\"\n",
    "        assert data.x.dtype in (torch.float32, torch.float64), \"Node features 'x' must be floating point.\"\n",
    "        assert data.edge_index.max() < data.num_nodes, \"'edge_index' contains invalid node indices.\"\n",
    "        return True\n",
    "    except AssertionError as e:\n",
    "        print(f\"Data check failed: {e}\")\n",
    "        return False"
   ],
   "id": "e5c36b0c09aa6625",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.844183Z",
     "start_time": "2024-10-12T10:20:31.840375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def neo_to_pyg_homogeneous(\n",
    "        data,\n",
    "        node_attr: str,\n",
    "):\n",
    "    if not data:\n",
    "        return None, None\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    relationships = data[\"relationships\"]\n",
    "\n",
    "    #print(f\"Nodes: {len(nodes)}, Relationships: {len(relationships)}\")\n",
    "\n",
    "    # Create a PyG Data object\n",
    "    pyg_data = PairData()\n",
    "\n",
    "    node_features = []\n",
    "    node_ids = []\n",
    "    node_id_map = {}\n",
    "\n",
    "    for node in nodes:\n",
    "        node_id = node.get(\"id\")\n",
    "        node_feature = node.get(node_attr, None)\n",
    "        if node_feature is None:\n",
    "            print(f\"Node {node_id} has no attribute {node_attr}\")\n",
    "            continue\n",
    "\n",
    "        # Map node id to its index in the list\n",
    "        idx = len(node_ids)\n",
    "        node_id_map[node_id] = idx\n",
    "        node_ids.append(node_id)\n",
    "\n",
    "        # Convert node features to tensors\n",
    "        node_feature_tensor = torch.tensor(node_feature, dtype=torch.float32)\n",
    "        node_features.append(node_feature_tensor)\n",
    "\n",
    "    # Convert list of features to tensor\n",
    "    if node_features:\n",
    "        pyg_data.x = torch.vstack(node_features)\n",
    "        pyg_data.num_nodes = pyg_data.x.size(0)\n",
    "    else:\n",
    "        print(\"No node features available.\")\n",
    "        return None, None\n",
    "\n",
    "    # Process relationships\n",
    "    edge_index_list = [[], []]\n",
    "\n",
    "    for rel in relationships:\n",
    "        source_id = rel.start_node.get(\"id\")\n",
    "        target_id = rel.end_node.get(\"id\")\n",
    "\n",
    "        if source_id not in node_id_map or target_id not in node_id_map:\n",
    "            print(f\"Edge from {source_id} to {target_id} cannot be mapped to node indices.\")\n",
    "            continue\n",
    "\n",
    "        source_idx = node_id_map[source_id]\n",
    "        target_idx = node_id_map[target_id]\n",
    "\n",
    "        edge_index_list[0].append(source_idx)\n",
    "        edge_index_list[1].append(target_idx)\n",
    "\n",
    "    # Convert edge lists to tensor\n",
    "    if edge_index_list[0] and edge_index_list[1]:\n",
    "        pyg_data.edge_index = torch.tensor(edge_index_list, dtype=torch.long)\n",
    "    else:\n",
    "        print(\"No edges available.\")\n",
    "        return None, None\n",
    "\n",
    "    return pyg_data, node_id_map"
   ],
   "id": "9f549c0dc7623d66",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Harvest Positive and Negative tuples from the graph database",
   "id": "b9eda2497b302be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.897206Z",
     "start_time": "2024-10-12T10:20:31.891603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataHarvester:\n",
    "    def __init__(self, db: DatabaseWrapper, gs: GraphSampling):\n",
    "        self.db = db\n",
    "        self.gs = gs\n",
    "        self.pairs = []\n",
    "        self.prepare_pairs()\n",
    "\n",
    "    def prepare_pairs(self):\n",
    "        print(\"Preparing pairs...\")\n",
    "        try:\n",
    "            print(\"Loading pairs...\")\n",
    "            self.load_pairs('./data/pairs.json')\n",
    "            print(f\"Loaded {len(self.pairs)} pairs.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Could not load pairs from file. Generating pairs...\")\n",
    "            self.generate_pairs()\n",
    "            print(f\"Generated {len(self.pairs)} pairs.\")\n",
    "            print(\"Saving pairs...\")\n",
    "            self.save_pairs('./data/pairs.json')\n",
    "            print(\"Pairs saved.\")\n",
    "            \n",
    "    def load_pairs(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.pairs = json.load(f)\n",
    "    \n",
    "    def save_pairs(self, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.pairs, f)\n",
    "                \n",
    "    def generate_pairs(self):\n",
    "        # Filter out the papers that are not present in the graph or have less than 2 edges\n",
    "        paper_ids = []\n",
    "        print(\"Checking data validity...\")\n",
    "        for nodes in self.db.iter_nodes_with_edge_count(NodeType.PUBLICATION, model_edge_type, ['id', 'true_author']):\n",
    "            for node in nodes:\n",
    "                data = gs.n_hop_neighbourhood(NodeType.PUBLICATION, node['id'], max_level=1)\n",
    "                data = neo_to_pyg_homogeneous(data, model_node_feature)[0]\n",
    "                if not graph_data_valid(data):\n",
    "                    continue\n",
    "                paper_ids.append(node['id'])\n",
    "        \n",
    "        print(f\"Total papers: {len(paper_ids)}\")\n",
    "        print(\"Preparing pairs...\")\n",
    "        paper_set = set(paper_ids)\n",
    "        \n",
    "        author_data = WhoIsWhoDataset.parse_train()\n",
    "        for author_id, data in author_data.items():\n",
    "            for key in data:\n",
    "                data[key] = [p_id for p_id in data[key] if p_id in paper_set]\n",
    "        \n",
    "        # Generate pairs with labels\n",
    "        pairs = []\n",
    "        \n",
    "        for author_id, data in author_data.items():\n",
    "            normal_data = data.get('normal_data', [])\n",
    "            outliers = data.get('outliers', [])\n",
    "                    \n",
    "            # Positive pairs: combinations of normal_data\n",
    "            pos_pairs = list(combinations(normal_data, 2))\n",
    "            if len(pos_pairs) > 50:\n",
    "                pos_pairs = random.sample(pos_pairs, 50)\n",
    "            for pair in pos_pairs:\n",
    "                pairs.append((pair[0], pair[1], 1))\n",
    "            \n",
    "            # Negative pairs: product of normal_data and outliers\n",
    "            neg_pairs = list(product(normal_data, outliers))\n",
    "            if len(neg_pairs) > 50:\n",
    "                neg_pairs = random.sample(neg_pairs, 50)\n",
    "            elif len(neg_pairs) < len(pos_pairs):\n",
    "                # Sample random paper ids from other authors\n",
    "                while len(neg_pairs) < len(pos_pairs):\n",
    "                    p1 = random.choice(normal_data)\n",
    "                    p2 = random.choice(paper_ids)\n",
    "                    if p2 not in normal_data:\n",
    "                        neg_pairs.append((p1, p2))\n",
    "            for pair in neg_pairs:\n",
    "                pairs.append((pair[0], pair[1], 0))\n",
    "        \n",
    "        print(f\"Total pairs: {len(pairs)}. Done.\")\n",
    "        self.pairs = pairs\n",
    "                "
   ],
   "id": "5b8e23ac5ebacd35",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GAT Encoder",
   "id": "b1431a2c7d9c9e59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.935643Z",
     "start_time": "2024-10-12T10:20:31.933474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class GATEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(-1, hidden_channels, heads=num_heads)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * num_heads, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ],
   "id": "c0ebc105a776068a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:31.979583Z",
     "start_time": "2024-10-12T10:20:31.977220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def contrastive_loss(embeddings1, embeddings2, labels, margin=1.0):\n",
    "    # Compute Euclidean distances between embeddings\n",
    "    distances = F.pairwise_distance(embeddings1, embeddings2)\n",
    "    \n",
    "    # Loss\n",
    "    loss_pos = labels * distances.pow(2)  # For positive pairs\n",
    "    loss_neg = (1 - labels) * F.relu(margin - distances).pow(2)  # For negative pairs\n",
    "    loss = loss_pos + loss_neg\n",
    "    return loss.mean()\n"
   ],
   "id": "1860d722d7a121d1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:32.023612Z",
     "start_time": "2024-10-12T10:20:32.021375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# custom collate function adjusted for GraphPairDataset\n",
    "\"\"\"def custom_collate(batch):\n",
    "    batch = [data for data in batch if data is not None]\n",
    "    if len(batch) == 0:\n",
    "        return Batch()\n",
    "    return Batch.from_data_list(batch)\"\"\"\n",
    "\n",
    "def custom_collate(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None  # Skip empty batches\n",
    "\n",
    "    data1_list = [item[0] for item in batch]\n",
    "    data2_list = [item[1] for item in batch]\n",
    "    \n",
    "    labels = torch.stack([item[2] for item in batch])\n",
    "    \n",
    "    batch1 = Batch.from_data_list(data1_list)\n",
    "    batch2 = Batch.from_data_list(data2_list)\n",
    "\n",
    "    return batch1, batch2, labels"
   ],
   "id": "1529ad8e8ee337ce",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:32.067946Z",
     "start_time": "2024-10-12T10:20:32.065300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot loss\n",
    "def plot_loss(losses, epoch_marker_pos=None, plot_title=\"Loss\", plot_file=None):\n",
    "    if plot_file is None:\n",
    "        plot_file = f'./data/losses/loss.png'\n",
    "    if epoch_marker_pos is None:\n",
    "        epoch_marker_pos = []\n",
    "        \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=f'Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    for ix, x_pos in enumerate(epoch_marker_pos):\n",
    "        plt.axvline(x=x_pos, color='red', linestyle='dotted', linewidth=1)\n",
    "        plt.text(\n",
    "        x_pos,\n",
    "        max(losses),\n",
    "        f'Epoch {ix}',\n",
    "        rotation=90,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        fontsize=10,\n",
    "        color='red'\n",
    "    )\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()"
   ],
   "id": "76a48fbe61da1a81",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:32.113449Z",
     "start_time": "2024-10-12T10:20:32.110256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, batch1, batch2, labels, optimizer, margin):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch1 = batch1.to(device)\n",
    "    batch2 = batch2.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    embeddings1 = model(batch1)\n",
    "    embeddings2 = model(batch2)\n",
    "\n",
    "    embeddings1_central = embeddings1[batch1.central_node_id]\n",
    "    embeddings2_central = embeddings2[batch2.central_node_id]\n",
    "\n",
    "    loss = contrastive_loss(embeddings1_central, embeddings2_central, labels, margin)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    batch_loss = loss.item()/len(labels)\n",
    "    #print(f\"Batch loss: {batch_loss:.4f}\")\n",
    "    return batch_loss\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for data1, data2, labels in dataloader:\n",
    "        if data1 is None or data2 is None:\n",
    "            continue  # Skip empty batches\n",
    "\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeddings1 = model(data1)[data1.central_node_id]\n",
    "        embeddings2 = model(data2)[data2.central_node_id]\n",
    "        \n",
    "        loss = contrastive_loss(embeddings1, embeddings2, labels, margin)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "    "
   ],
   "id": "2ff2cd9e6fd091ef",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training ",
   "id": "b67cf89d3d17cc12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:20:32.177113Z",
     "start_time": "2024-10-12T10:20:32.161036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DatabaseWrapper(database='homogeneous-graph')\n",
    "data_harvester = DataHarvester(db=db, gs=gs)\n",
    "\n",
    "\n",
    "# Split the pairs into train and test\n",
    "random.shuffle(data_harvester.pairs)\n",
    "train_size = int(0.95 * len(data_harvester.pairs))\n",
    "train_pairs = data_harvester.pairs[:train_size]\n",
    "test_pairs = data_harvester.pairs[train_size:]\n",
    "\n",
    "# Remove the train pairs from the test pairs\n",
    "train_dataset = GraphPairDataset(train_pairs, gs)\n",
    "test_dataset = GraphPairDataset(test_pairs, gs)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# Create model\n",
    "model = GATEncoder(128, 32).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ],
   "id": "62ef1028a5c5fd5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 12:20:32,162 - DatabaseWrapper - INFO - Connecting to the database ...\n",
      "2024-10-12 12:20:32,163 - DatabaseWrapper - INFO - Database ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing pairs...\n",
      "Loading pairs...\n",
      "Loaded 6184 pairs.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-12T10:20:32.211749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 20\n",
    "margin = 1.0  # Margin for contrastive loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_marker_pos = list(range(0, len(train_dataloader) * epoch, len(train_dataloader)))\n",
    "    \n",
    "    for data1, data2, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        if data1 is None or data2 is None:\n",
    "            continue\n",
    "        \n",
    "        if len(train_losses) % 10 == 0:\n",
    "            test_loss = test(model, test_dataloader)\n",
    "            test_losses.append(test_loss)\n",
    "            test_epoch_marker_pos = [marker/10 for marker in epoch_marker_pos if marker != 0]\n",
    "            plot_loss(test_losses, epoch_marker_pos=test_epoch_marker_pos, plot_title='Test Loss', plot_file=f'./data/losses/test_loss_homo_edges.png')\n",
    "            \n",
    "        loss = train(model, data1, data2, labels, optimizer, margin)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        plot_loss(train_losses, epoch_marker_pos=epoch_marker_pos, plot_title='Training Loss', plot_file=f'./data/losses/train_loss_homo_edges.png')\n",
    "    \n",
    "    # Save model if loss has decreased\n",
    "    if len(test_losses) > 1 and test_losses[-1] < min(test_losses[:-1]):\n",
    "        print(f\"Saving model at epoch {epoch}...\")\n",
    "        torch.save(model.state_dict(), f'./data/models/gat_encoder_homo_edges.pt')"
   ],
   "id": "c3b28cd52881796",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/184 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfe6af4722ab4a99b86e5cbe3a0fa1b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3545\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    distances = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch1, batch2, labels in dataloader:\n",
    "            batch1 = batch1.to(device)\n",
    "            batch2 = batch2.to(device)\n",
    "            embeddings1 = model(batch1)\n",
    "            embeddings2 = model(batch2)\n",
    "            dist = F.pairwise_distance(embeddings1, embeddings2).cpu().numpy()\n",
    "            distances.extend(dist)\n",
    "            labels_list.extend(labels.numpy())\n",
    "    return distances, labels_list\n",
    "\n",
    "# After training\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "distances, labels_list = evaluate(model, dataloader)\n",
    "roc_auc = roc_auc_score(labels_list, -np.array(distances))  # Negative distances for similarity\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ],
   "id": "1955b297484a6fa3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
