{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T17:20:42.086162Z",
     "start_time": "2024-11-25T17:20:39.709015Z"
    }
   },
   "source": [
    "from util_homogeneous import *\n",
    "from util import *\n",
    "from training_homogeneous import *\n",
    "from gat_models import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn.modules.loss import TripletMarginLoss\n",
    "\n",
    "from src.shared.database_wrapper import DatabaseWrapper\n",
    "from src.shared.graph_schema import *\n",
    "from src.shared.graph_sampling import GraphSampling\n",
    "\n",
    "random.seed(40)\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(40)\n",
    "torch.cuda.manual_seed_all(40)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincie/.anaconda3/envs/master/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "ee4fa502d7e51e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T17:20:42.095888Z",
     "start_time": "2024-11-25T17:20:42.091964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph sampling configurations\n",
    "node_spec = NodeType.PUBLICATION\n",
    "\n",
    "edge_spec = EdgeType.SIM_TITLE\n",
    "\n",
    "node_properties = [\n",
    "    'id',\n",
    "    'feature_vec',\n",
    "]\n",
    "\n",
    "database = 'dense-graph'\n",
    "gs = GraphSampling(\n",
    "    node_spec=[node_spec],\n",
    "    edge_spec=[edge_spec],\n",
    "    node_properties=node_properties,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "# Model configurations\n",
    "\n",
    "config = {\n",
    "    'experiment': 'GATv2 encoder (with linear layer + dropout) trained on homogeneous dense graph (publication nodes with title and abstract, title edges) using Triplet Loss and full embeddings',\n",
    "    'max_hops': 3,\n",
    "    'model_node_feature': 'feature_vec',  # Node feature to use for GAT encoder\n",
    "    'hidden_channels': 32,\n",
    "    'out_channels': 16,\n",
    "    'num_heads': 8,\n",
    "    'margin': 1.0,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 0.005,\n",
    "    'weight_decay': 5e-4,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "model_class = HomoGATEncoderLinearDropout\n",
    "loss_fn = TripletMarginLoss(margin=config['margin'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# TODO: Adjust result folder name!\n",
    "result_folder_name = 'dense (title) full_emb linear_layer dropout'\n",
    "result_folder_path = f'./data/results/{result_folder_name}'\n",
    "if not os.path.exists(result_folder_path):\n",
    "    os.mkdir(result_folder_path)"
   ],
   "id": "91b04efb689d61f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default edge type: SimilarTitle for homogeneous graph sampling.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T17:25:45.950369Z",
     "start_time": "2024-11-25T17:20:42.140271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DatabaseWrapper(database=database)\n",
    "data_harvester = TripletDataHarvester(db=db, gs=gs, edge_spec=[edge_spec], config=config, valid_triplets_save_file='valid_triplets_dense_title', transformer_model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Split the pairs into train and test\n",
    "\n",
    "train_size = int(0.85 * len(data_harvester.triplets))\n",
    "test_size = int(0.1 * len(data_harvester.triplets))\n",
    "eval_size = len(data_harvester.triplets) - train_size - test_size\n",
    "\n",
    "# Harvest the evaluation triplets first, since triplets are ordered by author. This will ensure that the evaluation set has authors not seen in the training set.\n",
    "eval_triplets = data_harvester.triplets[:eval_size]\n",
    "\n",
    "train_test_triplets = data_harvester.triplets[eval_size:]\n",
    "random.shuffle(train_test_triplets)\n",
    "\n",
    "train_triplets = train_test_triplets[:train_size]\n",
    "test_triplets = train_test_triplets[train_size:]\n",
    "config['train_size'] = len(train_triplets)\n",
    "config['test_size'] = len(test_triplets)\n",
    "config['eval_size'] = len(eval_triplets)\n",
    "\n",
    "print(f\"Train size: {len(train_triplets)}, Test size: {len(test_triplets)}, Eval size: {len(eval_triplets)}\")\n",
    "\n",
    "# Create the datasets from the pairs (distinct pairs for training and testing)\n",
    "train_dataset = HomogeneousGraphTripletDataset(train_triplets, gs, config=config)\n",
    "test_dataset = HomogeneousGraphTripletDataset(test_triplets, gs, config=config)\n",
    "eval_dataset = HomogeneousGraphTripletDataset(eval_triplets, gs, config=config)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=custom_triplet_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=custom_triplet_collate)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=custom_triplet_collate)\n",
    "\n",
    "# Create model\n",
    "metadata = (\n",
    "    node_spec.value,\n",
    "    edge_spec.value\n",
    ")\n",
    "config['node_spec'] = metadata[0]\n",
    "config['edge_spec'] = metadata[1]\n",
    "model = model_class(config['hidden_channels'], config['out_channels'], num_heads=config['num_heads']).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])"
   ],
   "id": "62ef1028a5c5fd5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 18:20:42,143 - DatabaseWrapper - INFO - Connecting to the database ...\n",
      "2024-11-25 18:20:42,143 - DatabaseWrapper - INFO - Database ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing triplets...\n",
      "Loading triplets...\n",
      "Could not load triplets from file. Generating triplets...\n",
      "Checking data validity...\n",
      "Out of 20034 checked papers, 10726 are valid and 9308 are invalid.\n",
      "Generating hard triplets ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincie/.anaconda3/envs/master/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets generated: 9755. Done.\n",
      "Generated 9755 triplets.\n",
      "Saving triplets...\n",
      "Triplets saved.\n",
      "Train size: 8291, Test size: 975, Eval size: 489\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-25T17:25:46.018511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = config['num_epochs']\n",
    "train_losses = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_correct_pos = []\n",
    "test_correct_neg = []\n",
    "\n",
    "eval_losses = []\n",
    "eval_accuracies = []\n",
    "eval_correct_pos = []\n",
    "eval_correct_neg = []\n",
    "\n",
    "current_batch = 1\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"=== Epoch {epoch}/{num_epochs} ======================\")\n",
    "    epoch_marker_pos = list(range(0, len(train_dataloader) * epoch, len(train_dataloader)))\n",
    "    current_batch = 1\n",
    "    for batch_anchor, batch_pos, batch_neg in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        if batch_anchor is None or batch_pos is None or batch_neg is None:\n",
    "            continue\n",
    "        \n",
    "        if current_batch == 1 or current_batch == len(train_dataloader) // 2:\n",
    "            print(f\"___ Current Batch {current_batch}/{len(train_dataloader)} _________________________\")\n",
    "            # Model testing\n",
    "            print(\"    Test Results:\")\n",
    "            test_loss, test_num_correct, test_correct_pos_val, test_correct_neg_val = test(\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=test_dataloader,\n",
    "                margin=config['margin']\n",
    "            )\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_num_correct)\n",
    "            test_correct_pos.append(test_correct_pos_val)\n",
    "            test_correct_neg.append(test_correct_neg_val)\n",
    "    \n",
    "            plot_loss(test_losses, epoch_len=2, plot_title='Test Loss', plot_avg=False, plot_file=result_folder_path + '/test_loss.png')\n",
    "            plot_loss(\n",
    "                test_accuracies,\n",
    "                epoch_len=2,\n",
    "                plot_title='Test Accuracy',\n",
    "                plot_avg=False, \n",
    "                x_label='Test Iterations',\n",
    "                y_label='Accuracy',\n",
    "                line_label='Accuracy',\n",
    "                plot_file=result_folder_path + '/test_accuracy.png'\n",
    "            )\n",
    "            \n",
    "            # Model evaluation\n",
    "            print(\"    Eval Results:\")\n",
    "            eval_loss, eval_num_correct, eval_correct_pos_val, eval_correct_neg_val = evaluate(\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                dataloader=eval_dataloader,\n",
    "                margin=config['margin']\n",
    "            )\n",
    "            eval_losses.append(eval_loss)\n",
    "            eval_accuracies.append(eval_num_correct)\n",
    "            eval_correct_pos.append(eval_correct_pos_val)\n",
    "            eval_correct_neg.append(eval_correct_neg_val)\n",
    "            \n",
    "            plot_loss(eval_losses, epoch_len=2, plot_title='Evaluation Loss', plot_avg=False, plot_file=result_folder_path + '/eval_loss.png')\n",
    "            plot_loss(\n",
    "                eval_accuracies, \n",
    "                epoch_len=2, \n",
    "                plot_title='Evaluation Accuracy', \n",
    "                plot_avg=False, \n",
    "                x_label='Eval Iterations',\n",
    "                y_label='Accuracy',\n",
    "                line_label='Accuracy',\n",
    "                plot_file=result_folder_path + '/eval_accuracy.png'\n",
    "            )\n",
    "            \n",
    "        loss = train(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            batch_anchor=batch_anchor,\n",
    "            batch_pos=batch_pos,\n",
    "            batch_neg=batch_neg,\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        plot_loss(train_losses, epoch_len=len(train_dataloader), plot_title='Training Loss', plot_avg=True, plot_file=result_folder_path + '/train_loss.png')\n",
    "        current_batch += 1\n",
    "        \n",
    "    # Save config and training results\n",
    "    eval_results = {\n",
    "        'eval_losses': eval_losses,\n",
    "        'eval_accuracies': eval_accuracies,\n",
    "        'eval_correct_pos': eval_correct_pos,\n",
    "        'eval_correct_neg': eval_correct_neg\n",
    "    }\n",
    "    save_training_results(train_losses, test_losses, eval_results, config, result_folder_path + '/training_data.json')\n",
    "    \n",
    "    # Save model if loss has decreased\n",
    "    if len(test_losses) > 1 and test_losses[-1] < min(test_losses[:-1]):\n",
    "        print(f\"Saving model at epoch {epoch}...\")\n",
    "        torch.save(model.state_dict(), result_folder_path + '/gat_encoder.pt')"
   ],
   "id": "c3b28cd52881796",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/10 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bf9f05711d5418fbca33e58383e9c21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Current Batch 1/260 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 975 (100.00%), Correct negative: 0 (0.00%)\n",
      "        Total correct: 975 (50.00%)\n",
      "        Test Loss: 1.0283, Test Accuracy: 0.5000\n",
      "    Eval Results:\n",
      "        Correct positive: 489 (100.00%), Correct negative: 0 (0.00%)\n",
      "        Total correct: 489 (50.00%)\n",
      "        Eval Loss: 1.0002, Eval Accuracy: 0.5000\n",
      "___ Current Batch 130/260 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 711 (72.92%), Correct negative: 860 (88.21%)\n",
      "        Total correct: 1571 (80.56%)\n",
      "        Test Loss: 0.2582, Test Accuracy: 0.8056\n",
      "    Eval Results:\n",
      "        Correct positive: 249 (50.92%), Correct negative: 171 (34.97%)\n",
      "        Total correct: 420 (42.94%)\n",
      "        Eval Loss: 1.2149, Eval Accuracy: 0.4294\n",
      "Saving model at epoch 1...\n",
      "=== Epoch 2/10 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62cef8b151b24f6080e5a05c4e69a7aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Current Batch 1/260 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 565 (57.95%), Correct negative: 881 (90.36%)\n",
      "        Total correct: 1446 (74.15%)\n",
      "        Test Loss: 0.2686, Test Accuracy: 0.7415\n",
      "    Eval Results:\n",
      "        Correct positive: 224 (45.81%), Correct negative: 194 (39.67%)\n",
      "        Total correct: 418 (42.74%)\n",
      "        Eval Loss: 1.3621, Eval Accuracy: 0.4274\n",
      "___ Current Batch 130/260 _________________________\n",
      "    Test Results:\n",
      "        Correct positive: 605 (62.05%), Correct negative: 898 (92.10%)\n",
      "        Total correct: 1503 (77.08%)\n",
      "        Test Loss: 0.2185, Test Accuracy: 0.7708\n",
      "    Eval Results:\n",
      "        Correct positive: 261 (53.37%), Correct negative: 203 (41.51%)\n",
      "        Total correct: 464 (47.44%)\n",
      "        Eval Loss: 1.0898, Eval Accuracy: 0.4744\n",
      "Saving model at epoch 2...\n",
      "=== Epoch 3/10 ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/260 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5eab803b32a742019b08746923ed397b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ Current Batch 1/260 _________________________\n",
      "    Test Results:\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
